{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from generation import MarkedIntensityHomogenuosPoisson, generate_samples_marked\n",
    "\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        self.max_len = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        padded_seq = np.zeros((self.max_len, 2))\n",
    "        padded_seq[:len(seq)] = np.array(seq)\n",
    "        seq_len = len(seq)\n",
    "        \n",
    "        return {\n",
    "            'sequence': torch.FloatTensor(padded_seq),\n",
    "            'length': torch.LongTensor([seq_len]),\n",
    "            'time_series': torch.ones(self.max_len, NUM_STEPS_TIMESERIES, TIMESERIES_FEATURE)\n",
    "        }\n",
    "\n",
    "DIM_SIZE = 7\n",
    "mi = MarkedIntensityHomogenuosPoisson(DIM_SIZE)\n",
    "for u in range(DIM_SIZE):\n",
    "    mi.initialize(1.0, u)\n",
    "simulated_sequences = generate_samples_marked(mi, 15.0, 1000)\n",
    "dataset = PointProcessDataset(simulated_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simulated_sequences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        # 直接将序列转换为tensor格式存储\n",
    "        self.sequences = [\n",
    "            torch.tensor([[event[0], event[1]] for event in seq], dtype=torch.float32)\n",
    "            for seq in sequences\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 直接返回原始序列，不进行padding\n",
    "        return self.sequences[idx]\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "DIM_SIZE = 7\n",
    "BATCH_SIZE=256\n",
    "mi = MarkedIntensityHomogenuosPoisson(DIM_SIZE)\n",
    "for u in range(DIM_SIZE):\n",
    "    mi.initialize(1.0, u)\n",
    "simulated_sequences = generate_samples_marked(mi, 15.0, 1000)\n",
    "dataset = PointProcessDataset(simulated_sequences)\n",
    "\n",
    "\n",
    "# 在创建DataLoader时使用自定义的collate_fn\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size_event, hidden_size_time, reg=0.1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg = reg\n",
    "        self.event_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        self.event_gru = nn.GRU(num_classes, hidden_size_event, batch_first=True)\n",
    "        self.time_gru = nn.GRU(1, hidden_size_time, batch_first=True)\n",
    "        combined_size = hidden_size_event + hidden_size_time\n",
    "        self.time_output = nn.Linear(combined_size, 1)\n",
    "        self.mark_output = nn.Linear(combined_size, num_classes)\n",
    "\n",
    "    def forward(self, event_sequence):\n",
    "        marks = event_sequence[..., 0].long()\n",
    "        times = event_sequence[..., 1].unsqueeze(-1)\n",
    "        \n",
    "        # 修正: 使用event_embedding处理marks\n",
    "        mark_embedded = self.event_embedding(marks)\n",
    "        event_output, _ = self.event_gru(mark_embedded)\n",
    "        time_output, _ = self.time_gru(times)\n",
    "        \n",
    "        combined_output = torch.cat([event_output, time_output], dim=-1)\n",
    "        time_pred = self.time_output(combined_output)\n",
    "        mark_logits = self.mark_output(combined_output)\n",
    "        return time_pred, mark_logits\n",
    "\n",
    "    def compute_loss(self, time_pred, mark_logits, targets):\n",
    "        true_times = targets[..., 1].unsqueeze(-1)\n",
    "        true_marks = targets[..., 0].long()\n",
    "        time_loss = F.mse_loss(time_pred, true_times)\n",
    "        mark_logits_flat = mark_logits.view(-1, self.num_classes)\n",
    "        true_marks_flat = true_marks.view(-1)\n",
    "        mark_loss = F.cross_entropy(mark_logits_flat, true_marks_flat)\n",
    "        \n",
    "        total_loss = mark_loss + self.reg * time_loss\n",
    "        \n",
    "        return total_loss, mark_loss, time_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/4 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 75\u001b[0m\n\u001b[1;32m     66\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleGRU(\n\u001b[1;32m     67\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mDIM_SIZE,\n\u001b[1;32m     68\u001b[0m     hidden_size_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     69\u001b[0m     hidden_size_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# 创建训练数据加载\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[126], line 44\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m batch_mark_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     43\u001b[0m batch_time_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m---> 44\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     46\u001b[0m train_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    训练模型的函数\n",
    "    \n",
    "    Args:\n",
    "        model: SimpleGRU模型实例\n",
    "        train_loader: 训练数据的DataLoader\n",
    "        num_epochs: 训练轮数\n",
    "        learning_rate: 学习率\n",
    "        device: 训练设备（'cuda'或'cpu'）\n",
    "    \"\"\"\n",
    "    print(f\"Training on {device}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        train_mark_loss_sum = 0\n",
    "        train_time_loss_sum = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            batch_mark_loss = 0\n",
    "            batch_time_loss = 0\n",
    "            \n",
    "            for sequence in batch:\n",
    "                sequence = sequence.to(device)\n",
    "                sequence = sequence.unsqueeze(0)\n",
    "                time_pred, mark_logits = model(sequence)\n",
    "                loss, mark_loss, time_loss = model.compute_loss(time_pred, mark_logits, sequence)\n",
    "                batch_loss += loss\n",
    "                batch_mark_loss += mark_loss\n",
    "                batch_time_loss += time_loss\n",
    "            batch_size = len(batch)\n",
    "            batch_loss /= batch_size\n",
    "            batch_mark_loss /= batch_size\n",
    "            batch_time_loss /= batch_size\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_total_loss += batch_loss.item()\n",
    "            train_mark_loss_sum += batch_mark_loss.item()\n",
    "            train_time_loss_sum += batch_time_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{batch_loss.item():.4f}',\n",
    "                'mark_loss': f'{batch_mark_loss.item():.4f}',\n",
    "                'time_loss': f'{batch_time_loss.item():.4f}'\n",
    "            })\n",
    "        avg_train_loss = train_total_loss / num_batches\n",
    "        avg_train_mark_loss = train_mark_loss_sum / num_batches\n",
    "        avg_train_time_loss = train_time_loss_sum / num_batches\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Mark: {avg_train_mark_loss:.4f}, Time: {avg_train_time_loss:.4f})')\n",
    "        print('-' * 80)\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建模型\n",
    "    model = SimpleGRU(\n",
    "        num_classes=DIM_SIZE,\n",
    "        hidden_size_event=16,\n",
    "        hidden_size_time=32\n",
    "    )\n",
    "    \n",
    "    # 创建训练数据加载\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=dataloader,\n",
    "        num_epochs=10,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "marked_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "################################### for neural network modeling\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenqing_liu/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 4999/4999 [00:15<00:00, 314.21it/s, loss=1.4620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Average Train Loss: 1.0540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 4999/4999 [00:15<00:00, 327.60it/s, loss=1.2577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20\n",
      "Average Train Loss: 1.0088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 4999/4999 [00:15<00:00, 332.41it/s, loss=1.4425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20\n",
      "Average Train Loss: 1.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 4999/4999 [00:15<00:00, 315.38it/s, loss=1.5970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20\n",
      "Average Train Loss: 1.0075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|██████████| 4999/4999 [00:16<00:00, 306.56it/s, loss=1.0575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20\n",
      "Average Train Loss: 1.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|██████████| 4999/4999 [00:17<00:00, 280.02it/s, loss=0.8609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20\n",
      "Average Train Loss: 1.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|██████████| 4999/4999 [00:17<00:00, 290.16it/s, loss=1.6346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20\n",
      "Average Train Loss: 1.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]:  77%|███████▋  | 3832/4999 [00:13<00:04, 283.86it/s, loss=0.8539]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 217\u001b[0m\n\u001b[1;32m    209\u001b[0m model \u001b[38;5;241m=\u001b[39m RNNPointProcess(\n\u001b[1;32m    210\u001b[0m     time_step\u001b[38;5;241m=\u001b[39mtime_step,\n\u001b[1;32m    211\u001b[0m     size_rnn\u001b[38;5;241m=\u001b[39msize_rnn,\n\u001b[1;32m    212\u001b[0m     size_nn\u001b[38;5;241m=\u001b[39msize_nn,\n\u001b[1;32m    213\u001b[0m     size_layer_chfn\u001b[38;5;241m=\u001b[39msize_layer_chfn\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[1;32m    225\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrnn_point_process.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[133], line 163\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# 检查loss是否为nan\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss):\n\u001b[0;32m--> 163\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# 添加梯度裁剪\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 生成训练和测试数据\n",
    "T_train = np.random.exponential(size=80000).cumsum()\n",
    "T_test = np.random.exponential(size=20000).cumsum()\n",
    "\n",
    "# 计算标准化参数\n",
    "mu = np.log(np.ediff1d(T_train)).mean()\n",
    "sigma = np.log(np.ediff1d(T_train)).std()\n",
    "\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, times, time_step):\n",
    "        self.times = times\n",
    "        self.time_step = time_step\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.times) - self.time_step - 1\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # 获取历史事件序列\n",
    "        history = self.times[idx:idx+self.time_step]\n",
    "        history = np.diff(history)  # 计算事件间隔\n",
    "        history = history.reshape(-1, 1)\n",
    "        \n",
    "        # 获取经过的时间\n",
    "        elapsed = self.times[idx+self.time_step+1] - self.times[idx+self.time_step]\n",
    "        \n",
    "        return {\n",
    "            'history': torch.FloatTensor(history),\n",
    "            'elapsed': torch.FloatTensor([elapsed]).clone()\n",
    "        }\n",
    "class RNNPointProcess(nn.Module):\n",
    "    def __init__(self, time_step=20, size_rnn=64, size_nn=64, size_layer_chfn=2):\n",
    "        super(RNNPointProcess, self).__init__()\n",
    "        self.time_step = time_step\n",
    "        self.size_rnn = size_rnn\n",
    "        self.size_nn = size_nn\n",
    "        \n",
    "        # RNN层\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size=size_rnn,\n",
    "            batch_first=True,\n",
    "            nonlinearity='tanh'  # 明确指定非线性函数\n",
    "        )\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        self.elapsed_time_linear = nn.Linear(1, size_nn, bias=False)\n",
    "        self.rnn_linear = nn.Linear(size_rnn, size_nn)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(size_nn, size_nn)\n",
    "            for _ in range(size_layer_chfn-1)\n",
    "        ])\n",
    "        \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(size_nn, 1)\n",
    "        \n",
    "        # 初始化为正权重\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # 注册标准化参数\n",
    "        self.register_buffer('mu', torch.tensor(mu, dtype=torch.float32))\n",
    "        self.register_buffer('sigma', torch.tensor(sigma, dtype=torch.float32))\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 使用较小的初始值\n",
    "            nn.init.uniform_(module.weight, 0, 0.1)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, event_history, elapsed_time):\n",
    "        # 添加数值稳定性检查\n",
    "        event_history = torch.clamp(event_history, min=1e-6)\n",
    "        elapsed_time = torch.clamp(elapsed_time, min=1e-6)\n",
    "        elapsed_time = elapsed_time.requires_grad_(True)\n",
    "        \n",
    "        # 标准化输入\n",
    "        event_history_norm = (torch.log(event_history) - self.mu) / self.sigma\n",
    "        elapsed_time_norm = (torch.log(elapsed_time) - self.mu) / self.sigma\n",
    "        \n",
    "        # 检查并处理无效值\n",
    "        event_history_norm = torch.nan_to_num(event_history_norm, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        elapsed_time_norm = torch.nan_to_num(elapsed_time_norm, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        # RNN处理历史序列\n",
    "        rnn_out, _ = self.rnn(event_history_norm)\n",
    "        rnn_out = rnn_out[:, -1, :]\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        hidden_tau = self.elapsed_time_linear(elapsed_time_norm)\n",
    "        hidden_rnn = self.rnn_linear(rnn_out)\n",
    "        hidden = torch.tanh(hidden_tau + hidden_rnn)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = torch.tanh(layer(hidden))\n",
    "        \n",
    "        # 计算累积危险函数（添加数值稳定性）\n",
    "        Int_l = F.softplus(self.output_layer(hidden)) + 1e-6\n",
    "        \n",
    "        # 计算危险函数\n",
    "        try:\n",
    "            l = torch.autograd.grad(Int_l.sum(), elapsed_time, create_graph=True)[0]\n",
    "            # 确保l是正的且有界\n",
    "            l = torch.clamp(l, min=1e-6, max=1e6)\n",
    "        except RuntimeError:\n",
    "            print(\"Gradient computation failed\")\n",
    "            l = torch.ones_like(elapsed_time) * 1e-6\n",
    "        \n",
    "        return l, Int_l\n",
    "    \n",
    "    def compute_loss(self, l, Int_l):\n",
    "        # 添加数值稳定性\n",
    "        l = torch.clamp(l, min=1e-6)\n",
    "        Int_l = torch.clamp(Int_l, min=0.0)\n",
    "        \n",
    "        # 计算负对数似然，添加梯度裁剪\n",
    "        loss = -torch.mean(torch.log(l) - Int_l)\n",
    "        \n",
    "        # 检查loss是否为nan\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Warning: Loss is NaN!\")\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=l.device)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "def train_model(model, train_loader, test_loader=None, num_epochs=10, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # 添加学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in train_bar:\n",
    "            history = batch['history'].to(device)\n",
    "            elapsed = batch['elapsed'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            l, Int_l = model(history, elapsed)\n",
    "            loss = model.compute_loss(l, Int_l)\n",
    "            \n",
    "            # 检查loss是否为nan\n",
    "            if not torch.isnan(loss):\n",
    "                loss.backward()\n",
    "                # 添加梯度裁剪\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Average Train Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(avg_train_loss)\n",
    "\n",
    "# 修改超参数\n",
    "if __name__ == \"__main__\":\n",
    "    time_step = 20\n",
    "    size_rnn = 32  # 减小RNN大小\n",
    "    size_nn = 32   # 减小隐藏层大小\n",
    "    size_layer_chfn = 2\n",
    "    batch_size = 16  # 减小批量大小\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.0001  # 使用更小的学习率\n",
    "    \n",
    "    # 其余代码保持不变...\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = PointProcessDataset(T_train, time_step)\n",
    "    test_dataset = PointProcessDataset(T_test, time_step)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    model = RNNPointProcess(\n",
    "        time_step=time_step,\n",
    "        size_rnn=size_rnn,\n",
    "        size_nn=size_nn,\n",
    "        size_layer_chfn=size_layer_chfn\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'rnn_point_process.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 186.17it/s, loss=1.7882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Average Train Loss: 2.4767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 200.08it/s, loss=1.2375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20\n",
      "Average Train Loss: 1.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 195.60it/s, loss=0.8734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20\n",
      "Average Train Loss: 1.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 195.06it/s, loss=1.0262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20\n",
      "Average Train Loss: 1.1403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]:  52%|█████▏    | 162/312 [00:00<00:00, 196.18it/s, loss=1.0139]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 218\u001b[0m\n\u001b[1;32m    210\u001b[0m model \u001b[38;5;241m=\u001b[39m GRUPointProcess(\n\u001b[1;32m    211\u001b[0m     time_step\u001b[38;5;241m=\u001b[39mtime_step,\n\u001b[1;32m    212\u001b[0m     size_gru\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    213\u001b[0m     size_nn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    214\u001b[0m     size_layer_chfn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    215\u001b[0m )\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\n\u001b[1;32m    224\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[135], line 160\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    158\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# 移除梯度裁剪\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    163\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/adam.py:367\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    364\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 367\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n\u001b[1;32m    370\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 数据集类\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, data, time_step=20):\n",
    "        self.data = data\n",
    "        self.time_step = time_step\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.time_step\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        history = self.data[idx:idx+self.time_step].reshape(-1, 1)\n",
    "        elapsed = self.data[idx+self.time_step].reshape(-1)\n",
    "        \n",
    "        return {\n",
    "            'history': torch.FloatTensor(history),\n",
    "            'elapsed': torch.FloatTensor(elapsed)\n",
    "        }\n",
    "\n",
    "class GRUPointProcess(nn.Module):\n",
    "    def __init__(self, time_step=20, size_gru=64, size_nn=64, size_layer_chfn=2):\n",
    "        super(GRUPointProcess, self).__init__()\n",
    "        self.time_step = time_step\n",
    "        self.size_gru = size_gru\n",
    "        self.size_nn = size_nn\n",
    "        \n",
    "        # GRU层\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=size_gru,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        self.elapsed_time_linear = nn.Linear(1, size_nn, bias=False)\n",
    "        self.gru_linear = nn.Linear(size_gru, size_nn)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(size_nn, size_nn)\n",
    "            for _ in range(size_layer_chfn-1)\n",
    "        ])\n",
    "        \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(size_nn, 1)\n",
    "        \n",
    "        # 初始化为正权重\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # 注册标准化参数\n",
    "        self.register_buffer('mu', torch.tensor(mu, dtype=torch.float32))\n",
    "        self.register_buffer('sigma', torch.tensor(sigma, dtype=torch.float32))\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 使用较小的初始值\n",
    "            nn.init.uniform_(module.weight, 0, 0.1)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, event_history, elapsed_time):\n",
    "        # 添加数值稳定性检查\n",
    "        event_history = torch.clamp(event_history, min=1e-6)\n",
    "        elapsed_time = torch.clamp(elapsed_time, min=1e-6)\n",
    "        elapsed_time = elapsed_time.requires_grad_(True)\n",
    "        \n",
    "        # 标准化输入\n",
    "        event_history_norm = (torch.log(event_history) - self.mu) / self.sigma\n",
    "        elapsed_time_norm = (torch.log(elapsed_time) - self.mu) / self.sigma\n",
    "        \n",
    "        # 检查并处理无效值\n",
    "        event_history_norm = torch.nan_to_num(event_history_norm, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        elapsed_time_norm = torch.nan_to_num(elapsed_time_norm, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        # GRU处理历史序列\n",
    "        gru_out, _ = self.gru(event_history_norm)\n",
    "        gru_out = gru_out[:, -1, :]  # 取最后一个时间步的输出\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        hidden_tau = self.elapsed_time_linear(elapsed_time_norm)\n",
    "        hidden_gru = self.gru_linear(gru_out)\n",
    "        hidden = torch.tanh(hidden_tau + hidden_gru)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = torch.tanh(layer(hidden))\n",
    "        \n",
    "        # 计算累积危险函数（添加数值稳定性）\n",
    "        Int_l = F.softplus(self.output_layer(hidden)) + 1e-6\n",
    "        \n",
    "        # 计算危险函数\n",
    "        try:\n",
    "            l = torch.autograd.grad(Int_l.sum(), elapsed_time, create_graph=True)[0]\n",
    "            # 确保l是正的且有界\n",
    "            l = torch.clamp(l, min=1e-6, max=1e6)\n",
    "        except RuntimeError:\n",
    "            print(\"Gradient computation failed\")\n",
    "            l = torch.ones_like(elapsed_time) * 1e-6\n",
    "        \n",
    "        return l, Int_l\n",
    "    \n",
    "    def compute_loss(self, l, Int_l):\n",
    "        # 添加数值稳定性\n",
    "        l = torch.clamp(l, min=1e-6)\n",
    "        Int_l = torch.clamp(Int_l, min=0.0)\n",
    "        \n",
    "        # 计算负对数似然，添加梯度裁剪\n",
    "        loss = -torch.mean(torch.log(l) - Int_l)\n",
    "        \n",
    "        # 检查loss是否为nan\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Warning: Loss is NaN!\")\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=l.device)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "def train_model(model, train_loader, test_loader=None, num_epochs=10, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # 添加学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in train_bar:\n",
    "            history = batch['history'].to(device)\n",
    "            elapsed = batch['elapsed'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            l, Int_l = model(history, elapsed)\n",
    "            loss = model.compute_loss(l, Int_l)\n",
    "            \n",
    "            # 检查loss是否为nan\n",
    "            if not torch.isnan(loss):\n",
    "                loss.backward()\n",
    "                # 移除梯度裁剪\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Average Train Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(avg_train_loss)\n",
    "        \n",
    "        # 保存模型\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f'gru_point_process_epoch_{epoch+1}.pt')\n",
    "# 生成训练数据\n",
    "def generate_data(num_samples, lambda_=1.0):\n",
    "    return np.random.exponential(1/lambda_, num_samples)\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成数据\n",
    "    num_samples = 10000\n",
    "    T_train = generate_data(num_samples)\n",
    "    T_test = generate_data(num_samples // 10)\n",
    "    \n",
    "    # 计算标准化参数\n",
    "    mu = np.mean(np.log(T_train))\n",
    "    sigma = np.std(np.log(T_train))\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    time_step = 20\n",
    "    batch_size = 32\n",
    "    train_dataset = PointProcessDataset(T_train, time_step)\n",
    "    test_dataset = PointProcessDataset(T_test, time_step)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    model = GRUPointProcess(\n",
    "        time_step=time_step,\n",
    "        size_gru=32,\n",
    "        size_nn=32,\n",
    "        size_layer_chfn=2\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        num_epochs=20,\n",
    "        learning_rate=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenqing_liu/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 182.09it/s, loss=2.1096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Average Train Loss: 2.0911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 187.04it/s, loss=1.8564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20\n",
      "Average Train Loss: 1.8369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 205.60it/s, loss=1.8494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20\n",
      "Average Train Loss: 1.7539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 199.71it/s, loss=1.8517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20\n",
      "Average Train Loss: 1.7271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 202.49it/s, loss=1.8450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20\n",
      "Average Train Loss: 1.7144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 200.63it/s, loss=1.8373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20\n",
      "Average Train Loss: 1.7057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 202.44it/s, loss=1.8311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20\n",
      "Average Train Loss: 1.7007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 199.97it/s, loss=1.8264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20\n",
      "Average Train Loss: 1.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 203.79it/s, loss=1.8234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20\n",
      "Average Train Loss: 1.6951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 195.05it/s, loss=1.8211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20\n",
      "Average Train Loss: 1.6929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]:  57%|█████▋    | 178/312 [00:00<00:00, 199.47it/s, loss=1.7709]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 219\u001b[0m\n\u001b[1;32m    211\u001b[0m     model \u001b[38;5;241m=\u001b[39m GRUPointProcess(\n\u001b[1;32m    212\u001b[0m         time_step\u001b[38;5;241m=\u001b[39mtime_step,\n\u001b[1;32m    213\u001b[0m         size_gru\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    214\u001b[0m         size_nn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    215\u001b[0m         size_layer_chfn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# 检查数据形状\u001b[39;00m\n\u001b[1;32m    228\u001b[0m sample_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n",
      "Cell \u001b[0;32mIn[153], line 151\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    148\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    150\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43melapsed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43melapsed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:697\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/autograd/profiler.py:738\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_enter_new(\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\n\u001b[1;32m    735\u001b[0m     )\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit:\n\u001b[1;32m    740\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 生成训练数据\n",
    "def generate_data(num_samples, lambda_=0.5):\n",
    "    # 生成指数分布的时间间隔\n",
    "    intervals = np.random.exponential(1/lambda_, num_samples)\n",
    "    # 计算累积时间\n",
    "    times = np.cumsum(intervals)\n",
    "    return times\n",
    "\n",
    "# 数据集类\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, data, time_step=20):\n",
    "        # 确保数据是按时间排序的\n",
    "        self.data = np.sort(data)\n",
    "        self.time_step = time_step\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.time_step\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # 获取时间序列窗口\n",
    "        history = self.data[idx:idx+self.time_step].reshape(-1, 1)\n",
    "        next_time = self.data[idx+self.time_step]\n",
    "        \n",
    "        # 计算时间差（elapsed time）\n",
    "        elapsed = next_time - history[-1, 0]\n",
    "        \n",
    "        return {\n",
    "            'history': torch.FloatTensor(history),\n",
    "            'elapsed': torch.FloatTensor([elapsed])\n",
    "        }\n",
    "\n",
    "class GRUPointProcess(nn.Module):\n",
    "    def __init__(self, time_step=20, size_gru=64, size_nn=64, size_layer_chfn=2):\n",
    "        super(GRUPointProcess, self).__init__()\n",
    "        self.time_step = time_step\n",
    "        self.size_gru = size_gru\n",
    "        self.size_nn = size_nn\n",
    "        \n",
    "        # GRU层\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=size_gru,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        self.elapsed_time_linear = nn.Linear(1, size_nn, bias=False)\n",
    "        self.gru_linear = nn.Linear(size_gru, size_nn)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(size_nn, size_nn)\n",
    "            for _ in range(size_layer_chfn-1)\n",
    "        ])\n",
    "        \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(size_nn, 1)\n",
    "        \n",
    "        # 初始化为正权重\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 使用较小的初始值\n",
    "            nn.init.uniform_(module.weight, 0, 0.1)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, event_history, elapsed_time):\n",
    "        # 添加数值稳定性检查\n",
    "        event_history = torch.clamp(event_history, min=1e-6)\n",
    "        elapsed_time = torch.clamp(elapsed_time, min=1e-6)\n",
    "        elapsed_time = elapsed_time.requires_grad_(True)\n",
    "        \n",
    "        # 标准化输入\n",
    "        event_history_norm = event_history\n",
    "        elapsed_time_norm = elapsed_time\n",
    "        \n",
    "        # GRU处理历史序列\n",
    "        gru_out, _ = self.gru(event_history_norm)\n",
    "        gru_out = gru_out[:, -1, :]  # 取最后一个时间步的输出\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        hidden_tau = self.elapsed_time_linear(elapsed_time_norm)\n",
    "        hidden_gru = self.gru_linear(gru_out)\n",
    "        hidden = torch.tanh(hidden_tau + hidden_gru)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = torch.tanh(layer(hidden))\n",
    "        \n",
    "        # 计算累积危险函数（添加数值稳定性）\n",
    "        Int_l = F.softplus(self.output_layer(hidden)) + 1e-6\n",
    "        \n",
    "        # 计算危险函数\n",
    "        try:\n",
    "            l = torch.autograd.grad(Int_l.sum(), elapsed_time, create_graph=True)[0]\n",
    "            # 确保l是正的且有界\n",
    "            l = torch.clamp(l, min=1e-6, max=1e6)\n",
    "        except RuntimeError:\n",
    "            print(\"Gradient computation failed\")\n",
    "            l = torch.ones_like(elapsed_time) * 1e-6\n",
    "        \n",
    "        return l, Int_l\n",
    "    \n",
    "    def compute_loss(self, l, Int_l):\n",
    "        # 添加数值稳定性\n",
    "        l = torch.clamp(l, min=1e-6)\n",
    "        Int_l = torch.clamp(Int_l, min=0.0)\n",
    "        \n",
    "        # 计算负对数似然\n",
    "        loss = -torch.mean(torch.log(l) - Int_l)\n",
    "        \n",
    "        # 检查loss是否为nan\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Warning: Loss is NaN!\")\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=l.device)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "def train_model(model, train_loader, test_loader=None, num_epochs=10, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # 添加学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in train_bar:\n",
    "            history = batch['history'].to(device)\n",
    "            elapsed = batch['elapsed'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            l, Int_l = model(history, elapsed)\n",
    "            loss = model.compute_loss(l, Int_l)\n",
    "            \n",
    "            # 检查loss是否为nan\n",
    "            if not torch.isnan(loss):\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Average Train Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(avg_train_loss)\n",
    "        \n",
    "        # 保存模型\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f'gru_point_process_epoch_{epoch+1}.pt')\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    num_samples = 10000\n",
    "    T_train = generate_data(num_samples)\n",
    "    T_test = generate_data(num_samples // 10)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    time_step = 20\n",
    "    batch_size = 32\n",
    "    train_dataset = PointProcessDataset(T_train, time_step)\n",
    "    test_dataset = PointProcessDataset(T_test, time_step)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False  # 不打乱顺序，保持时间序列的连续性\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    model = GRUPointProcess(\n",
    "        time_step=time_step,\n",
    "        size_gru=32,\n",
    "        size_nn=32,\n",
    "        size_layer_chfn=2\n",
    "    )\n",
    "    \n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        num_epochs=20,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "\n",
    "# 检查数据形状\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nData shapes:\")\n",
    "print(\"History shape:\", sample_batch['history'].shape)\n",
    "print(\"Elapsed shape:\", sample_batch['elapsed'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_synthetic_data(n_events=100, lambda_rate=0.5):\n",
    "    inter_event_times = np.random.exponential(scale=1/lambda_rate, size=n_events)\n",
    "    event_times = np.cumsum(inter_event_times)\n",
    "    return event_times\n",
    "event_times = generate_synthetic_data(n_events=200)\n",
    "\n",
    "# 定义数据集\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, data, time_step=20):\n",
    "        self.data = data\n",
    "        self.time_step = time_step\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.time_step\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        history = self.data[idx:idx+self.time_step].reshape(-1, 1)\n",
    "        next_time = self.data[idx+self.time_step]\n",
    "        elapsed = next_time - history[-1, 0]\n",
    "        return {\n",
    "            'history': torch.FloatTensor(history),\n",
    "            'elapsed': torch.FloatTensor([elapsed])\n",
    "        }\n",
    "\n",
    "# 数据准备\n",
    "time_step = 20\n",
    "dataset = PointProcessDataset(event_times, time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient computation failed\n",
      "Shape of predicted intensity (l): torch.Size([2, 1])\n",
      "Shape of cumulative intensity (Int_l): torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the GRUPointProcess model\n",
    "class GRUPointProcess(nn.Module):\n",
    "    def __init__(self, time_step=4, size_gru=64, size_nn=64, size_layer_chfn=4):\n",
    "        super(GRUPointProcess, self).__init__()\n",
    "        self.time_step = time_step\n",
    "        self.size_gru = size_gru\n",
    "        self.size_nn = size_nn\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=size_gru,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.elapsed_time_linear = nn.Linear(1, size_nn, bias=False)\n",
    "        self.gru_linear = nn.Linear(size_gru, size_nn)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(size_nn, size_nn)\n",
    "            for _ in range(size_layer_chfn-1)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(size_nn, 1)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.uniform_(module.weight, 0, 0.1)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, event_history, elapsed_time):\n",
    "        event_history = torch.clamp(event_history, min=1e-6)\n",
    "        elapsed_time = torch.clamp(elapsed_time, min=1e-6)\n",
    "        elapsed_time = elapsed_time.requires_grad_(True)\n",
    "        gru_out, _ = self.gru(event_history)\n",
    "        gru_out = gru_out[:, -1, :]\n",
    "        hidden_tau = self.elapsed_time_linear(elapsed_time)\n",
    "        hidden_gru = self.gru_linear(gru_out)\n",
    "        hidden = torch.tanh(hidden_tau + hidden_gru)\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = torch.tanh(layer(hidden))\n",
    "        Int_l = F.softplus(self.output_layer(hidden))\n",
    "        try:\n",
    "            l = torch.autograd.grad(Int_l, elapsed_time, create_graph=True)[0]\n",
    "            l = torch.clamp(l, min=1e-6, max=1e6)\n",
    "        except RuntimeError:\n",
    "            print(\"Gradient computation failed\")\n",
    "            l = torch.ones_like(elapsed_time) * 1e-6\n",
    "        return l, Int_l\n",
    "    \n",
    "    def compute_loss(self, l, Int_l):\n",
    "        l = torch.clamp(l, min=1e-6)\n",
    "        Int_l = torch.clamp(Int_l, min=0.0)\n",
    "        loss = -torch.mean(torch.log(l) - Int_l)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Warning: Loss is NaN!\")\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=l.device)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "# Create some dummy data\n",
    "event_history = torch.randn(2, 4, 1)  # 2 samples, 4 time steps, 1 feature\n",
    "elapsed_time = torch.randn(2, 1)  # 2 samples, 1 elapsed time value\n",
    "\n",
    "# Instantiate and run the model\n",
    "model = GRUPointProcess(time_step=4, size_gru=64, size_nn=64, size_layer_chfn=2)\n",
    "l, Int_l = model(event_history, elapsed_time)\n",
    "\n",
    "# Print the output shapes\n",
    "print(f\"Shape of predicted intensity (l): {l.shape}\")\n",
    "print(f\"Shape of cumulative intensity (Int_l): {Int_l.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
