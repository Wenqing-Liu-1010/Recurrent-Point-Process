{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from generation import MarkedIntensityHomogenuosPoisson, generate_samples_marked\n",
    "\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        self.max_len = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        padded_seq = np.zeros((self.max_len, 2))\n",
    "        padded_seq[:len(seq)] = np.array(seq)\n",
    "        seq_len = len(seq)\n",
    "        \n",
    "        return {\n",
    "            'sequence': torch.FloatTensor(padded_seq),\n",
    "            'length': torch.LongTensor([seq_len]),\n",
    "            'time_series': torch.ones(self.max_len, NUM_STEPS_TIMESERIES, TIMESERIES_FEATURE)\n",
    "        }\n",
    "\n",
    "DIM_SIZE = 7\n",
    "mi = MarkedIntensityHomogenuosPoisson(DIM_SIZE)\n",
    "for u in range(DIM_SIZE):\n",
    "    mi.initialize(1.0, u)\n",
    "simulated_sequences = generate_samples_marked(mi, 15.0, 1000)\n",
    "dataset = PointProcessDataset(simulated_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simulated_sequences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        # 直接将序列转换为tensor格式存储\n",
    "        self.sequences = [\n",
    "            torch.tensor([[event[0], event[1]] for event in seq], dtype=torch.float32)\n",
    "            for seq in sequences\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 直接返回原始序列，不进行padding\n",
    "        return self.sequences[idx]\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "DIM_SIZE = 7\n",
    "BATCH_SIZE=256\n",
    "mi = MarkedIntensityHomogenuosPoisson(DIM_SIZE)\n",
    "for u in range(DIM_SIZE):\n",
    "    mi.initialize(1.0, u)\n",
    "simulated_sequences = generate_samples_marked(mi, 15.0, 1000)\n",
    "dataset = PointProcessDataset(simulated_sequences)\n",
    "\n",
    "\n",
    "# 在创建DataLoader时使用自定义的collate_fn\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size_event, hidden_size_time, reg=0.1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg = reg\n",
    "        self.event_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        self.event_gru = nn.GRU(num_classes, hidden_size_event, batch_first=True)\n",
    "        self.time_gru = nn.GRU(1, hidden_size_time, batch_first=True)\n",
    "        combined_size = hidden_size_event + hidden_size_time\n",
    "        self.time_output = nn.Linear(combined_size, 1)\n",
    "        self.mark_output = nn.Linear(combined_size, num_classes)\n",
    "\n",
    "    def forward(self, event_sequence):\n",
    "        marks = event_sequence[..., 0].long()\n",
    "        times = event_sequence[..., 1].unsqueeze(-1)\n",
    "        \n",
    "        # 修正: 使用event_embedding处理marks\n",
    "        mark_embedded = self.event_embedding(marks)\n",
    "        event_output, _ = self.event_gru(mark_embedded)\n",
    "        time_output, _ = self.time_gru(times)\n",
    "        \n",
    "        combined_output = torch.cat([event_output, time_output], dim=-1)\n",
    "        time_pred = self.time_output(combined_output)\n",
    "        mark_logits = self.mark_output(combined_output)\n",
    "        return time_pred, mark_logits\n",
    "\n",
    "    def compute_loss(self, time_pred, mark_logits, targets):\n",
    "        true_times = targets[..., 1].unsqueeze(-1)\n",
    "        true_marks = targets[..., 0].long()\n",
    "        time_loss = F.mse_loss(time_pred, true_times)\n",
    "        mark_logits_flat = mark_logits.view(-1, self.num_classes)\n",
    "        true_marks_flat = true_marks.view(-1)\n",
    "        mark_loss = F.cross_entropy(mark_logits_flat, true_marks_flat)\n",
    "        \n",
    "        total_loss = mark_loss + self.reg * time_loss\n",
    "        \n",
    "        return total_loss, mark_loss, time_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 4/4 [00:22<00:00,  5.52s/it, loss=8.7964, mark_loss=1.9855, time_loss=68.1082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 - 22.09s\n",
      "Train Loss: 8.8937 (Mark: 1.9917, Time: 69.0204)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 4/4 [00:20<00:00,  5.13s/it, loss=8.4259, mark_loss=1.9702, time_loss=64.5570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10 - 20.53s\n",
      "Train Loss: 8.5696 (Mark: 1.9736, Time: 65.9599)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 4/4 [00:21<00:00,  5.34s/it, loss=8.1609, mark_loss=1.9536, time_loss=62.0733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10 - 21.36s\n",
      "Train Loss: 8.2578 (Mark: 1.9582, Time: 62.9961)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 4/4 [00:20<00:00,  5.16s/it, loss=7.8092, mark_loss=1.9403, time_loss=58.6896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10 - 20.63s\n",
      "Train Loss: 7.9512 (Mark: 1.9448, Time: 60.0643)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 4/4 [00:20<00:00,  5.18s/it, loss=7.5525, mark_loss=1.9286, time_loss=56.2385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10 - 20.70s\n",
      "Train Loss: 7.6516 (Mark: 1.9326, Time: 57.1898)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 4/4 [00:21<00:00,  5.33s/it, loss=7.2936, mark_loss=1.9172, time_loss=53.7640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10 - 21.34s\n",
      "Train Loss: 7.3569 (Mark: 1.9214, Time: 54.3549)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 4/4 [00:22<00:00,  5.58s/it, loss=6.9781, mark_loss=1.9073, time_loss=50.7074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10 - 22.31s\n",
      "Train Loss: 7.0648 (Mark: 1.9109, Time: 51.5392)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 4/4 [00:21<00:00,  5.41s/it, loss=6.6628, mark_loss=1.8971, time_loss=47.6572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10 - 21.64s\n",
      "Train Loss: 6.7755 (Mark: 1.9010, Time: 48.7452)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 4/4 [00:20<00:00,  5.09s/it, loss=6.3574, mark_loss=1.8879, time_loss=44.6946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10 - 20.37s\n",
      "Train Loss: 6.4891 (Mark: 1.8917, Time: 45.9741)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 4/4 [00:20<00:00,  5.15s/it, loss=6.0773, mark_loss=1.8790, time_loss=41.9826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10 - 20.59s\n",
      "Train Loss: 6.2075 (Mark: 1.8824, Time: 43.2509)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    训练模型的函数\n",
    "    \n",
    "    Args:\n",
    "        model: SimpleGRU模型实例\n",
    "        train_loader: 训练数据的DataLoader\n",
    "        num_epochs: 训练轮数\n",
    "        learning_rate: 学习率\n",
    "        device: 训练设备（'cuda'或'cpu'）\n",
    "    \"\"\"\n",
    "    print(f\"Training on {device}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        train_mark_loss_sum = 0\n",
    "        train_time_loss_sum = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            batch_mark_loss = 0\n",
    "            batch_time_loss = 0\n",
    "            \n",
    "            for sequence in batch:\n",
    "                sequence = sequence.to(device)\n",
    "                sequence = sequence.unsqueeze(0)\n",
    "                time_pred, mark_logits = model(sequence)\n",
    "                loss, mark_loss, time_loss = model.compute_loss(time_pred, mark_logits, sequence)\n",
    "                batch_loss += loss\n",
    "                batch_mark_loss += mark_loss\n",
    "                batch_time_loss += time_loss\n",
    "            batch_size = len(batch)\n",
    "            batch_loss /= batch_size\n",
    "            batch_mark_loss /= batch_size\n",
    "            batch_time_loss /= batch_size\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_total_loss += batch_loss.item()\n",
    "            train_mark_loss_sum += batch_mark_loss.item()\n",
    "            train_time_loss_sum += batch_time_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{batch_loss.item():.4f}',\n",
    "                'mark_loss': f'{batch_mark_loss.item():.4f}',\n",
    "                'time_loss': f'{batch_time_loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        # 计算并打印epoch的平均损失\n",
    "        avg_train_loss = train_total_loss / num_batches\n",
    "        avg_train_mark_loss = train_mark_loss_sum / num_batches\n",
    "        avg_train_time_loss = train_time_loss_sum / num_batches\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Mark: {avg_train_mark_loss:.4f}, Time: {avg_train_time_loss:.4f})')\n",
    "        print('-' * 80)\n",
    "\n",
    "# 使用示例：\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建模型\n",
    "    model = SimpleGRU(\n",
    "        num_classes=DIM_SIZE,\n",
    "        hidden_size_event=16,\n",
    "        hidden_size_time=32\n",
    "    )\n",
    "    \n",
    "    # 创建训练数据加载\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=dataloader,\n",
    "        num_epochs=10,\n",
    "        learning_rate=0.001\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
