{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from generation import MarkedIntensityHomogenuosPoisson, generate_samples_marked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = [\n",
    "            torch.tensor([[event[0], event[1]] for event in seq], dtype=torch.float32)\n",
    "            for seq in sequences\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "DIM_SIZE = 7\n",
    "BATCH_SIZE=256\n",
    "mi = MarkedIntensityHomogenuosPoisson(DIM_SIZE)\n",
    "for u in range(DIM_SIZE):\n",
    "    mi.initialize(1.0, u)\n",
    "simulated_sequences = generate_samples_marked(mi, 20.0, 10000)\n",
    "dataset = PointProcessDataset(simulated_sequences)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size_event, hidden_size_time, reg=0.1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg = reg\n",
    "        self.event_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        self.event_gru = nn.GRU(num_classes, hidden_size_event, batch_first=True)\n",
    "        self.time_gru = nn.GRU(1, hidden_size_time, batch_first=True)\n",
    "        combined_size = hidden_size_event + hidden_size_time\n",
    "        self.time_output = nn.Linear(combined_size, 1)\n",
    "        self.mark_output = nn.Linear(combined_size, num_classes)\n",
    "\n",
    "    def forward(self, event_sequence):\n",
    "        marks = event_sequence[..., 0].long()\n",
    "        times = event_sequence[..., 1].unsqueeze(-1)\n",
    "        mark_embedded = self.event_embedding(marks)\n",
    "        event_output, _ = self.event_gru(mark_embedded)\n",
    "        time_output, _ = self.time_gru(times)\n",
    "        combined_output = torch.cat([event_output, time_output], dim=-1)\n",
    "        time_pred = self.time_output(combined_output)\n",
    "        mark_logits = self.mark_output(combined_output)\n",
    "        return time_pred, mark_logits\n",
    "    def compute_loss(self, time_pred, mark_logits, targets):\n",
    "        true_times = targets[..., 1].unsqueeze(-1)\n",
    "        true_marks = targets[..., 0].long()\n",
    "        time_loss = F.mse_loss(time_pred, true_times)\n",
    "        mark_logits_flat = mark_logits.view(-1, self.num_classes)\n",
    "        true_marks_flat = true_marks.view(-1)\n",
    "        mark_loss = F.cross_entropy(mark_logits_flat, true_marks_flat)\n",
    "        total_loss = mark_loss + self.reg * time_loss\n",
    "        return total_loss, mark_loss, time_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s, loss=11.5613, mark_loss=1.8686, time_loss=96.9274] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100 - 15.80s\n",
      "Train Loss: 13.6339 (Mark: 1.9144, Time: 117.1947)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s, loss=7.9704, mark_loss=1.7912, time_loss=61.7925] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/100 - 15.61s\n",
      "Train Loss: 9.4751 (Mark: 1.8251, Time: 76.4998)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=5.6050, mark_loss=1.7466, time_loss=38.5845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/100 - 15.92s\n",
      "Train Loss: 6.5905 (Mark: 1.7791, Time: 48.1141)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 40/40 [00:15<00:00,  2.54it/s, loss=4.2511, mark_loss=1.5640, time_loss=26.8707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/100 - 15.73s\n",
      "Train Loss: 4.8883 (Mark: 1.6622, Time: 32.2607)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 40/40 [00:16<00:00,  2.48it/s, loss=3.3359, mark_loss=1.3476, time_loss=19.8830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/100 - 16.16s\n",
      "Train Loss: 3.7368 (Mark: 1.4582, Time: 22.7860)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=2.5397, mark_loss=1.1251, time_loss=14.1461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/100 - 15.97s\n",
      "Train Loss: 2.8590 (Mark: 1.2296, Time: 16.2939)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=1.9301, mark_loss=0.9410, time_loss=9.8906] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/100 - 15.89s\n",
      "Train Loss: 2.2090 (Mark: 1.0228, Time: 11.8621)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 40/40 [00:15<00:00,  2.54it/s, loss=1.5241, mark_loss=0.7967, time_loss=7.2737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/100 - 15.75s\n",
      "Train Loss: 1.7347 (Mark: 0.8618, Time: 8.7290)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 40/40 [00:16<00:00,  2.45it/s, loss=1.2360, mark_loss=0.6854, time_loss=5.5068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/100 - 16.33s\n",
      "Train Loss: 1.3904 (Mark: 0.7398, Time: 6.5054)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 40/40 [00:16<00:00,  2.49it/s, loss=1.0310, mark_loss=0.6072, time_loss=4.2375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/100 - 16.04s\n",
      "Train Loss: 1.1363 (Mark: 0.6425, Time: 4.9382)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 40/40 [00:16<00:00,  2.49it/s, loss=0.8442, mark_loss=0.5227, time_loss=3.2154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/100 - 16.09s\n",
      "Train Loss: 0.9419 (Mark: 0.5572, Time: 3.8467)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 40/40 [00:16<00:00,  2.47it/s, loss=0.7181, mark_loss=0.4271, time_loss=2.9100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/100 - 16.19s\n",
      "Train Loss: 0.7787 (Mark: 0.4695, Time: 3.0923)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=0.5950, mark_loss=0.3657, time_loss=2.2934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/100 - 15.92s\n",
      "Train Loss: 0.6402 (Mark: 0.3927, Time: 2.4746)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 40/40 [00:15<00:00,  2.50it/s, loss=0.4894, mark_loss=0.3148, time_loss=1.7459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/100 - 15.98s\n",
      "Train Loss: 0.5318 (Mark: 0.3340, Time: 1.9779)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s, loss=0.4183, mark_loss=0.2711, time_loss=1.4716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/100 - 15.65s\n",
      "Train Loss: 0.4474 (Mark: 0.2889, Time: 1.5858)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 40/40 [00:15<00:00,  2.58it/s, loss=0.3489, mark_loss=0.2338, time_loss=1.1511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/100 - 15.53s\n",
      "Train Loss: 0.3790 (Mark: 0.2510, Time: 1.2800)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.2959, mark_loss=0.2052, time_loss=0.9076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/100 - 15.84s\n",
      "Train Loss: 0.3235 (Mark: 0.2188, Time: 1.0464)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 40/40 [00:16<00:00,  2.49it/s, loss=0.2581, mark_loss=0.1776, time_loss=0.8055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/100 - 16.09s\n",
      "Train Loss: 0.2788 (Mark: 0.1917, Time: 0.8705)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 40/40 [00:16<00:00,  2.46it/s, loss=0.2241, mark_loss=0.1586, time_loss=0.6549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/100 - 16.23s\n",
      "Train Loss: 0.2426 (Mark: 0.1692, Time: 0.7336)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 40/40 [00:16<00:00,  2.43it/s, loss=0.2010, mark_loss=0.1413, time_loss=0.5966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/100 - 16.47s\n",
      "Train Loss: 0.2128 (Mark: 0.1502, Time: 0.6259)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=0.1664, mark_loss=0.1219, time_loss=0.4446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/100 - 17.21s\n",
      "Train Loss: 0.1852 (Mark: 0.1322, Time: 0.5298)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s, loss=0.1434, mark_loss=0.1043, time_loss=0.3911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/100 - 15.78s\n",
      "Train Loss: 0.1577 (Mark: 0.1132, Time: 0.4451)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s, loss=0.1223, mark_loss=0.0890, time_loss=0.3327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/100 - 15.82s\n",
      "Train Loss: 0.1335 (Mark: 0.0961, Time: 0.3736)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 40/40 [00:16<00:00,  2.49it/s, loss=0.1089, mark_loss=0.0761, time_loss=0.3278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/100 - 16.07s\n",
      "Train Loss: 0.1143 (Mark: 0.0824, Time: 0.3194)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 40/40 [00:16<00:00,  2.48it/s, loss=0.0940, mark_loss=0.0674, time_loss=0.2662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/100 - 16.12s\n",
      "Train Loss: 0.0994 (Mark: 0.0717, Time: 0.2775)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 40/40 [00:16<00:00,  2.44it/s, loss=0.0856, mark_loss=0.0604, time_loss=0.2523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/100 - 16.41s\n",
      "Train Loss: 0.0879 (Mark: 0.0636, Time: 0.2432)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 40/40 [00:15<00:00,  2.50it/s, loss=0.0742, mark_loss=0.0550, time_loss=0.1918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/100 - 15.99s\n",
      "Train Loss: 0.0784 (Mark: 0.0571, Time: 0.2131)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 40/40 [00:16<00:00,  2.47it/s, loss=0.0688, mark_loss=0.0498, time_loss=0.1905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/100 - 16.17s\n",
      "Train Loss: 0.0707 (Mark: 0.0517, Time: 0.1895)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 40/40 [00:16<00:00,  2.48it/s, loss=0.0610, mark_loss=0.0460, time_loss=0.1501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/100 - 16.11s\n",
      "Train Loss: 0.0642 (Mark: 0.0475, Time: 0.1675)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 40/40 [00:16<00:00,  2.46it/s, loss=0.0549, mark_loss=0.0428, time_loss=0.1205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/100 - 16.28s\n",
      "Train Loss: 0.0588 (Mark: 0.0440, Time: 0.1485)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.0517, mark_loss=0.0395, time_loss=0.1217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/100 - 15.85s\n",
      "Train Loss: 0.0542 (Mark: 0.0410, Time: 0.1328)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 40/40 [00:16<00:00,  2.45it/s, loss=0.0497, mark_loss=0.0377, time_loss=0.1197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/100 - 16.32s\n",
      "Train Loss: 0.0503 (Mark: 0.0383, Time: 0.1192)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|██████████| 40/40 [00:16<00:00,  2.44it/s, loss=0.0461, mark_loss=0.0348, time_loss=0.1135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/100 - 16.39s\n",
      "Train Loss: 0.0467 (Mark: 0.0360, Time: 0.1074)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.0430, mark_loss=0.0330, time_loss=0.0999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/100 - 15.87s\n",
      "Train Loss: 0.0436 (Mark: 0.0339, Time: 0.0969)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=0.0394, mark_loss=0.0312, time_loss=0.0820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/100 - 15.97s\n",
      "Train Loss: 0.0408 (Mark: 0.0320, Time: 0.0875)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|██████████| 40/40 [00:16<00:00,  2.50it/s, loss=0.0370, mark_loss=0.0297, time_loss=0.0722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/100 - 16.01s\n",
      "Train Loss: 0.0383 (Mark: 0.0303, Time: 0.0797)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|██████████| 40/40 [00:15<00:00,  2.54it/s, loss=0.0351, mark_loss=0.0279, time_loss=0.0726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/100 - 15.73s\n",
      "Train Loss: 0.0361 (Mark: 0.0288, Time: 0.0729)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s, loss=0.0322, mark_loss=0.0266, time_loss=0.0565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/100 - 15.61s\n",
      "Train Loss: 0.0340 (Mark: 0.0273, Time: 0.0665)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=0.0319, mark_loss=0.0253, time_loss=0.0664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/100 - 15.92s\n",
      "Train Loss: 0.0322 (Mark: 0.0260, Time: 0.0614)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.0301, mark_loss=0.0242, time_loss=0.0580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/100 - 15.88s\n",
      "Train Loss: 0.0304 (Mark: 0.0248, Time: 0.0563)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.0279, mark_loss=0.0230, time_loss=0.0486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/100 - 15.85s\n",
      "Train Loss: 0.0288 (Mark: 0.0237, Time: 0.0516)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=0.0263, mark_loss=0.0219, time_loss=0.0440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/100 - 15.93s\n",
      "Train Loss: 0.0274 (Mark: 0.0226, Time: 0.0475)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s, loss=0.0254, mark_loss=0.0214, time_loss=0.0403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/100 - 15.63s\n",
      "Train Loss: 0.0260 (Mark: 0.0216, Time: 0.0439)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.0241, mark_loss=0.0203, time_loss=0.0382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/100 - 15.90s\n",
      "Train Loss: 0.0248 (Mark: 0.0207, Time: 0.0405)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=0.0234, mark_loss=0.0195, time_loss=0.0387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/100 - 15.92s\n",
      "Train Loss: 0.0236 (Mark: 0.0199, Time: 0.0375)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|██████████| 40/40 [00:16<00:00,  2.48it/s, loss=0.0222, mark_loss=0.0187, time_loss=0.0355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/100 - 16.12s\n",
      "Train Loss: 0.0225 (Mark: 0.0191, Time: 0.0346)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s, loss=0.0211, mark_loss=0.0182, time_loss=0.0293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/100 - 15.84s\n",
      "Train Loss: 0.0215 (Mark: 0.0183, Time: 0.0319)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|██████████| 40/40 [00:16<00:00,  2.48it/s, loss=0.0200, mark_loss=0.0174, time_loss=0.0261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/100 - 16.14s\n",
      "Train Loss: 0.0206 (Mark: 0.0176, Time: 0.0296)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100:  15%|█▌        | 6/40 [00:02<00:16,  2.11it/s, loss=0.0199, mark_loss=0.0172, time_loss=0.0276]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 77\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# 创建模型\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     model \u001b[38;5;241m=\u001b[39m SimpleGRU(\n\u001b[1;32m     72\u001b[0m         num_classes\u001b[38;5;241m=\u001b[39mDIM_SIZE,\n\u001b[1;32m     73\u001b[0m         hidden_size_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     74\u001b[0m         hidden_size_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[0;32m---> 77\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 46\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m batch_mark_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     45\u001b[0m batch_time_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m---> 46\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m train_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # 导入 tqdm 函数\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    训练模型的函数\n",
    "    \n",
    "    Args:\n",
    "        model: SimpleGRU模型实例\n",
    "        train_loader: 训练数据的DataLoader\n",
    "        num_epochs: 训练轮数\n",
    "        learning_rate: 学习率\n",
    "        device: 训练设备（'cuda'或'cpu'）\n",
    "    \"\"\"\n",
    "    print(f\"Training on {device}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        train_mark_loss_sum = 0\n",
    "        train_time_loss_sum = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            batch_mark_loss = 0\n",
    "            batch_time_loss = 0\n",
    "            \n",
    "            for sequence in batch:\n",
    "                sequence = sequence.to(device)\n",
    "                sequence = sequence.unsqueeze(0)\n",
    "                time_pred, mark_logits = model(sequence)\n",
    "                loss, mark_loss, time_loss = model.compute_loss(time_pred, mark_logits, sequence)\n",
    "                batch_loss += loss\n",
    "                batch_mark_loss += mark_loss\n",
    "                batch_time_loss += time_loss\n",
    "            batch_size = len(batch)\n",
    "            batch_loss /= batch_size\n",
    "            batch_mark_loss /= batch_size\n",
    "            batch_time_loss /= batch_size\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_total_loss += batch_loss.item()\n",
    "            train_mark_loss_sum += batch_mark_loss.item()\n",
    "            train_time_loss_sum += batch_time_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{batch_loss.item():.4f}',\n",
    "                'mark_loss': f'{batch_mark_loss.item():.4f}',\n",
    "                'time_loss': f'{batch_time_loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_total_loss / num_batches\n",
    "        avg_train_mark_loss = train_mark_loss_sum / num_batches\n",
    "        avg_train_time_loss = train_time_loss_sum / num_batches\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Mark: {avg_train_mark_loss:.4f}, Time: {avg_train_time_loss:.4f})')\n",
    "        print('-' * 80)\n",
    "\n",
    "# 使用示例：\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建模型\n",
    "    model = SimpleGRU(\n",
    "        num_classes=DIM_SIZE,\n",
    "        hidden_size_event=16,\n",
    "        hidden_size_time=32\n",
    "    )\n",
    "    \n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=dataloader,\n",
    "        num_epochs=100,\n",
    "        learning_rate=0.001\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) >= 2:  # 确保序列至少有2个事件\n",
    "                # 将序列转换为tensor\n",
    "                seq_tensor = torch.tensor([[event[0], event[1]] for event in seq], dtype=torch.float32)\n",
    "                # 分离输入序列(x)和目标值(y)\n",
    "                x = seq_tensor[:-1]  # 除最后一个外的所有事件\n",
    "                y = seq_tensor[-1]   # 最后一个事件\n",
    "                self.sequences.append((x, y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch中的每个元素都是(x, y)对\n",
    "    return batch\n",
    "\n",
    "# 生成数据\n",
    "DIM_SIZE = 7\n",
    "BATCH_SIZE = 256\n",
    "mi = MarkedIntensityHomogenuosPoisson(DIM_SIZE)\n",
    "for u in range(DIM_SIZE):\n",
    "    mi.initialize(1.0, u)\n",
    "simulated_sequences = generate_samples_marked(mi, 20.0, 10000)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = PointProcessDataset(simulated_sequences)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size_event, hidden_size_time, reg=0.1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg = reg\n",
    "        self.event_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        self.event_gru = nn.GRU(num_classes, hidden_size_event, batch_first=True)\n",
    "        self.time_gru = nn.GRU(1, hidden_size_time, batch_first=True)\n",
    "        combined_size = hidden_size_event + hidden_size_time\n",
    "        self.time_output = nn.Linear(combined_size, 1)\n",
    "        self.mark_output = nn.Linear(combined_size, num_classes)\n",
    "\n",
    "    def forward(self, event_sequence):\n",
    "        marks = event_sequence[..., 0].long()\n",
    "        times = event_sequence[..., 1].unsqueeze(-1)\n",
    "        \n",
    "        # 处理所有时间步\n",
    "        mark_embedded = self.event_embedding(marks)\n",
    "        event_output, _ = self.event_gru(mark_embedded)\n",
    "        time_output, _ = self.time_gru(times)\n",
    "        \n",
    "        # 只取最后一个时间步的输出\n",
    "        combined_output = torch.cat([\n",
    "            event_output[:, -1:, :],  # 只取最后一步\n",
    "            time_output[:, -1:, :]    # 只取最后一步\n",
    "        ], dim=-1)\n",
    "        \n",
    "        time_pred = self.time_output(combined_output)\n",
    "        mark_logits = self.mark_output(combined_output)\n",
    "        return time_pred, mark_logits\n",
    "\n",
    "    def compute_loss(self, time_pred, mark_logits, targets):\n",
    "        # 只取最后一步的真实值\n",
    "        true_times = targets[..., -1:, 1].unsqueeze(-1)  # 最后一步的时间\n",
    "        true_marks = targets[..., -1, 0].long()          # 最后一步的标记\n",
    "        \n",
    "        # 计算损失\n",
    "        time_loss = F.mse_loss(time_pred, true_times)\n",
    "        mark_logits_flat = mark_logits.view(-1, self.num_classes)\n",
    "        true_marks_flat = true_marks.view(-1)\n",
    "        mark_loss = F.cross_entropy(mark_logits_flat, true_marks_flat)\n",
    "        \n",
    "        total_loss = mark_loss + self.reg * time_loss\n",
    "        return total_loss, mark_loss, time_loss\n",
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    print(f\"Training on {device}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        train_mark_loss_sum = 0\n",
    "        train_time_loss_sum = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            batch_mark_loss = 0\n",
    "            batch_time_loss = 0\n",
    "            \n",
    "            for x, y in batch:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                x = x.unsqueeze(0)  # 添加batch维度\n",
    "                y = y.unsqueeze(0)  # 添加batch维度\n",
    "                \n",
    "                # 前向传播\n",
    "                time_pred, mark_logits = model(x)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss, mark_loss, time_loss = model.compute_loss(time_pred, mark_logits, y)\n",
    "                \n",
    "                batch_loss += loss\n",
    "                batch_mark_loss += mark_loss\n",
    "                batch_time_loss += time_loss\n",
    "            \n",
    "            batch_size = len(batch)\n",
    "            batch_loss /= batch_size\n",
    "            batch_mark_loss /= batch_size\n",
    "            batch_time_loss /= batch_size\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_total_loss += batch_loss.item()\n",
    "            train_mark_loss_sum += batch_mark_loss.item()\n",
    "            train_time_loss_sum += batch_time_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{batch_loss.item():.4f}',\n",
    "                'mark_loss': f'{batch_mark_loss.item():.4f}',\n",
    "                'time_loss': f'{batch_time_loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_total_loss / num_batches\n",
    "        avg_train_mark_loss = train_mark_loss_sum / num_batches\n",
    "        avg_train_time_loss = train_time_loss_sum / num_batches\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Mark: {avg_train_mark_loss:.4f}, Time: {avg_train_time_loss:.4f})')\n",
    "        print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/40 [00:00<?, ?it/s]/tmp/ipykernel_897/3030971952.py:81: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  time_loss = F.mse_loss(time_pred, true_times)\n",
      "Epoch 1/20: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=31.2713, mark_loss=1.9048, time_loss=293.6648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20 - 17.27s\n",
      "Train Loss: 38.3748 (Mark: 1.9589, Time: 364.1587)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=18.8100, mark_loss=1.9452, time_loss=168.6483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20 - 16.75s\n",
      "Train Loss: 24.0759 (Mark: 1.9490, Time: 221.2686)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=12.6471, mark_loss=1.9403, time_loss=107.0675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20 - 17.04s\n",
      "Train Loss: 15.3684 (Mark: 1.9475, Time: 134.2088)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 40/40 [00:17<00:00,  2.29it/s, loss=8.4767, mark_loss=1.9214, time_loss=65.5524] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20 - 17.44s\n",
      "Train Loss: 10.2832 (Mark: 1.9484, Time: 83.3486)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 40/40 [00:16<00:00,  2.36it/s, loss=5.7599, mark_loss=1.9443, time_loss=38.1554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20 - 16.98s\n",
      "Train Loss: 6.9916 (Mark: 1.9488, Time: 50.4282)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=4.1817, mark_loss=1.9420, time_loss=22.3973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20 - 16.73s\n",
      "Train Loss: 4.8962 (Mark: 1.9470, Time: 29.4924)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 40/40 [00:17<00:00,  2.30it/s, loss=3.1453, mark_loss=1.9553, time_loss=11.8994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20 - 17.41s\n",
      "Train Loss: 3.6026 (Mark: 1.9491, Time: 16.5354)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 40/40 [00:17<00:00,  2.30it/s, loss=2.6063, mark_loss=1.9619, time_loss=6.4441] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20 - 17.37s\n",
      "Train Loss: 2.8365 (Mark: 1.9489, Time: 8.8765)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 40/40 [00:16<00:00,  2.35it/s, loss=2.2949, mark_loss=1.9733, time_loss=3.2159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20 - 16.99s\n",
      "Train Loss: 2.4052 (Mark: 1.9506, Time: 4.5455)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 40/40 [00:17<00:00,  2.34it/s, loss=2.1100, mark_loss=1.9587, time_loss=1.5127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20 - 17.10s\n",
      "Train Loss: 2.1698 (Mark: 1.9477, Time: 2.2215)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=2.0280, mark_loss=1.9535, time_loss=0.7450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/20 - 16.88s\n",
      "Train Loss: 2.0525 (Mark: 1.9484, Time: 1.0405)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=1.9570, mark_loss=1.9284, time_loss=0.2856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/20 - 17.00s\n",
      "Train Loss: 1.9969 (Mark: 1.9501, Time: 0.4683)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=1.9534, mark_loss=1.9391, time_loss=0.1428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/20 - 17.03s\n",
      "Train Loss: 1.9708 (Mark: 1.9500, Time: 0.2083)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=1.9402, mark_loss=1.9333, time_loss=0.0691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/20 - 16.66s\n",
      "Train Loss: 1.9572 (Mark: 1.9477, Time: 0.0956)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=1.9308, mark_loss=1.9263, time_loss=0.0451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/20 - 17.24s\n",
      "Train Loss: 1.9526 (Mark: 1.9477, Time: 0.0494)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 40/40 [00:16<00:00,  2.36it/s, loss=1.9539, mark_loss=1.9516, time_loss=0.0235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/20 - 16.93s\n",
      "Train Loss: 1.9525 (Mark: 1.9494, Time: 0.0311)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=1.9468, mark_loss=1.9441, time_loss=0.0274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/20 - 16.69s\n",
      "Train Loss: 1.9521 (Mark: 1.9497, Time: 0.0247)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=1.9644, mark_loss=1.9629, time_loss=0.0151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/20 - 17.24s\n",
      "Train Loss: 1.9509 (Mark: 1.9487, Time: 0.0222)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=1.9407, mark_loss=1.9393, time_loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/20 - 16.87s\n",
      "Train Loss: 1.9505 (Mark: 1.9484, Time: 0.0214)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=1.9477, mark_loss=1.9445, time_loss=0.0323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/20 - 16.69s\n",
      "Train Loss: 1.9512 (Mark: 1.9490, Time: 0.0216)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = SimpleGRU(\n",
    "    num_classes=DIM_SIZE,\n",
    "    hidden_size_event=32,\n",
    "    hidden_size_time=32,\n",
    "    reg=0.1\n",
    ")\n",
    "\n",
    "# 设置训练参数\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 训练模型\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=dataloader,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# # 可选：保存模型\n",
    "# torch.save(model.state_dict(), 'simple_gru_model.pth')\n",
    "\n",
    "# # 可选：预测示例\n",
    "# model.eval()  # 设置为评估模式\n",
    "# with torch.no_grad():\n",
    "#     # 获取一个样本\n",
    "#     x_sample, y_sample = next(iter(dataloader))[0]\n",
    "#     x_sample = x_sample.unsqueeze(0)  # 添加batch维度\n",
    "    \n",
    "#     # 预测\n",
    "#     time_pred, mark_logits = model(x_sample)\n",
    "    \n",
    "#     # 获取预测的标记类别\n",
    "#     predicted_mark = torch.argmax(mark_logits, dim=-1)\n",
    "    \n",
    "#     print(\"\\n预测示例:\")\n",
    "#     print(f\"实际值 - 标记: {y_sample[0].item()}, 时间: {y_sample[1].item():.4f}\")\n",
    "#     print(f\"预测值 - 标记: {predicted_mark.item()}, 时间: {time_pred.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "标记分布:\n",
      "标记 0: 10000.0 个样本 (100.00%)\n",
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/40 [00:00<?, ?it/s]/tmp/ipykernel_897/82287682.py:55: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  time_loss = F.mse_loss(time_pred, true_times)\n",
      "Epoch 1/50: 100%|██████████| 40/40 [00:17<00:00,  2.34it/s, loss=854.1581, mark_loss=0.0000, time_loss=8541.5811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 - 17.07s\n",
      "Train Loss: 930.8562 (Mark: 0.0000, Time: 9308.5615)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=737.1065, mark_loss=0.0000, time_loss=7371.0649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 - 16.70s\n",
      "Train Loss: 794.5440 (Mark: 0.0000, Time: 7945.4399)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=648.7274, mark_loss=0.0000, time_loss=6487.2725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 - 16.85s\n",
      "Train Loss: 692.9558 (Mark: 0.0000, Time: 6929.5579)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=566.6910, mark_loss=0.0000, time_loss=5666.9106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 - 17.15s\n",
      "Train Loss: 606.3584 (Mark: 0.0000, Time: 6063.5839)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=496.5091, mark_loss=0.0000, time_loss=4965.0903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 - 16.64s\n",
      "Train Loss: 529.7967 (Mark: 0.0000, Time: 5297.9669)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=433.0239, mark_loss=0.0000, time_loss=4330.2383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 - 17.23s\n",
      "Train Loss: 461.0504 (Mark: 0.0000, Time: 4610.5034)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=372.3096, mark_loss=0.0000, time_loss=3723.0957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 - 16.73s\n",
      "Train Loss: 400.2792 (Mark: 0.0000, Time: 4002.7918)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 40/40 [00:17<00:00,  2.34it/s, loss=320.3572, mark_loss=0.0000, time_loss=3203.5718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 - 17.10s\n",
      "Train Loss: 346.6920 (Mark: 0.0000, Time: 3466.9202)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 40/40 [00:17<00:00,  2.30it/s, loss=280.7843, mark_loss=0.0000, time_loss=2807.8435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 - 17.39s\n",
      "Train Loss: 299.3643 (Mark: 0.0000, Time: 2993.6431)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=240.4721, mark_loss=0.0000, time_loss=2404.7212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 - 17.02s\n",
      "Train Loss: 257.3524 (Mark: 0.0000, Time: 2573.5236)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 40/40 [00:16<00:00,  2.42it/s, loss=204.2225, mark_loss=0.0000, time_loss=2042.2250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 - 16.53s\n",
      "Train Loss: 220.2557 (Mark: 0.0000, Time: 2202.5573)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=171.1792, mark_loss=0.0000, time_loss=1711.7919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 - 16.82s\n",
      "Train Loss: 187.6014 (Mark: 0.0000, Time: 1876.0140)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 40/40 [00:16<00:00,  2.41it/s, loss=149.6371, mark_loss=0.0000, time_loss=1496.3708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 - 16.59s\n",
      "Train Loss: 159.1729 (Mark: 0.0000, Time: 1591.7288)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=125.1671, mark_loss=0.0000, time_loss=1251.6710]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50 - 16.72s\n",
      "Train Loss: 134.2412 (Mark: 0.0000, Time: 1342.4120)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=102.7341, mark_loss=0.0000, time_loss=1027.3405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 - 16.66s\n",
      "Train Loss: 112.5792 (Mark: 0.0000, Time: 1125.7923)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 40/40 [00:16<00:00,  2.42it/s, loss=86.8478, mark_loss=0.0000, time_loss=868.4777] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50 - 16.55s\n",
      "Train Loss: 93.9625 (Mark: 0.0000, Time: 939.6250)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=70.1366, mark_loss=0.0000, time_loss=701.3661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50 - 16.66s\n",
      "Train Loss: 77.9060 (Mark: 0.0000, Time: 779.0600)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 40/40 [00:17<00:00,  2.34it/s, loss=57.2267, mark_loss=0.0000, time_loss=572.2666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50 - 17.08s\n",
      "Train Loss: 64.2353 (Mark: 0.0000, Time: 642.3525)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=48.7136, mark_loss=0.0000, time_loss=487.1362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50 - 16.67s\n",
      "Train Loss: 52.6955 (Mark: 0.0000, Time: 526.9551)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=37.6529, mark_loss=0.0000, time_loss=376.5287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50 - 16.88s\n",
      "Train Loss: 42.8715 (Mark: 0.0000, Time: 428.7148)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=30.3359, mark_loss=0.0000, time_loss=303.3589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50 - 17.14s\n",
      "Train Loss: 34.6914 (Mark: 0.0000, Time: 346.9142)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=25.5730, mark_loss=0.0000, time_loss=255.7302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50 - 16.72s\n",
      "Train Loss: 27.9191 (Mark: 0.0000, Time: 279.1907)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=20.1069, mark_loss=0.0000, time_loss=201.0686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50 - 17.21s\n",
      "Train Loss: 22.2852 (Mark: 0.0000, Time: 222.8518)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=16.7291, mark_loss=0.0000, time_loss=167.2906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50 - 16.85s\n",
      "Train Loss: 17.6905 (Mark: 0.0000, Time: 176.9049)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=11.6511, mark_loss=0.0000, time_loss=116.5115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50 - 16.75s\n",
      "Train Loss: 13.8944 (Mark: 0.0000, Time: 138.9439)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=10.3248, mark_loss=0.0000, time_loss=103.2481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50 - 17.13s\n",
      "Train Loss: 10.8956 (Mark: 0.0000, Time: 108.9556)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 40/40 [00:17<00:00,  2.31it/s, loss=6.9217, mark_loss=0.0000, time_loss=69.2171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50 - 17.32s\n",
      "Train Loss: 8.4345 (Mark: 0.0000, Time: 84.3451)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=5.8484, mark_loss=0.0000, time_loss=58.4843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50 - 17.03s\n",
      "Train Loss: 6.5178 (Mark: 0.0000, Time: 65.1781)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 40/40 [00:17<00:00,  2.31it/s, loss=4.3127, mark_loss=0.0000, time_loss=43.1269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50 - 17.30s\n",
      "Train Loss: 4.9888 (Mark: 0.0000, Time: 49.8876)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=3.3905, mark_loss=0.0000, time_loss=33.9053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50 - 17.22s\n",
      "Train Loss: 3.7994 (Mark: 0.0000, Time: 37.9937)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=2.2414, mark_loss=0.0000, time_loss=22.4138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50 - 16.70s\n",
      "Train Loss: 2.8662 (Mark: 0.0000, Time: 28.6619)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 40/40 [00:16<00:00,  2.36it/s, loss=1.8880, mark_loss=0.0000, time_loss=18.8796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50 - 16.91s\n",
      "Train Loss: 2.1617 (Mark: 0.0000, Time: 21.6175)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 40/40 [00:16<00:00,  2.41it/s, loss=1.2874, mark_loss=0.0000, time_loss=12.8739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50 - 16.63s\n",
      "Train Loss: 1.6166 (Mark: 0.0000, Time: 16.1661)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 40/40 [00:16<00:00,  2.36it/s, loss=1.0074, mark_loss=0.0000, time_loss=10.0738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50 - 16.93s\n",
      "Train Loss: 1.2093 (Mark: 0.0000, Time: 12.0931)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=0.5666, mark_loss=0.0000, time_loss=5.6662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50 - 16.89s\n",
      "Train Loss: 0.8985 (Mark: 0.0000, Time: 8.9848)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.6241, mark_loss=0.0000, time_loss=6.2410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50 - 16.80s\n",
      "Train Loss: 0.6773 (Mark: 0.0000, Time: 6.7733)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 40/40 [00:17<00:00,  2.27it/s, loss=0.4454, mark_loss=0.0000, time_loss=4.4542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50 - 17.60s\n",
      "Train Loss: 0.5091 (Mark: 0.0000, Time: 5.0905)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=0.4361, mark_loss=0.0000, time_loss=4.3609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50 - 17.14s\n",
      "Train Loss: 0.3892 (Mark: 0.0000, Time: 3.8923)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 40/40 [00:16<00:00,  2.36it/s, loss=0.2379, mark_loss=0.0000, time_loss=2.3791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/50 - 16.93s\n",
      "Train Loss: 0.2981 (Mark: 0.0000, Time: 2.9814)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=0.1654, mark_loss=0.0000, time_loss=1.6543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/50 - 17.05s\n",
      "Train Loss: 0.2348 (Mark: 0.0000, Time: 2.3484)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=0.1743, mark_loss=0.0000, time_loss=1.7432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/50 - 16.85s\n",
      "Train Loss: 0.1916 (Mark: 0.0000, Time: 1.9163)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=0.1224, mark_loss=0.0000, time_loss=1.2241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/50 - 16.85s\n",
      "Train Loss: 0.1599 (Mark: 0.0000, Time: 1.5992)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.1005, mark_loss=0.0000, time_loss=1.0052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/50 - 16.81s\n",
      "Train Loss: 0.1384 (Mark: 0.0000, Time: 1.3843)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.0883, mark_loss=0.0000, time_loss=0.8831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/50 - 16.78s\n",
      "Train Loss: 0.1237 (Mark: 0.0000, Time: 1.2368)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=0.1994, mark_loss=0.0000, time_loss=1.9936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/50 - 16.89s\n",
      "Train Loss: 0.1166 (Mark: 0.0000, Time: 1.1660)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=0.1706, mark_loss=0.0000, time_loss=1.7058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/50 - 17.14s\n",
      "Train Loss: 0.1095 (Mark: 0.0000, Time: 1.0952)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.1070, mark_loss=0.0000, time_loss=1.0696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/50 - 16.82s\n",
      "Train Loss: 0.1039 (Mark: 0.0000, Time: 1.0385)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 40/40 [00:17<00:00,  2.28it/s, loss=0.0732, mark_loss=0.0000, time_loss=0.7318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/50 - 17.56s\n",
      "Train Loss: 0.1001 (Mark: 0.0000, Time: 1.0014)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.1994, mark_loss=0.0000, time_loss=1.9941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/50 - 16.81s\n",
      "Train Loss: 0.1012 (Mark: 0.0000, Time: 1.0116)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=0.0950, mark_loss=0.0000, time_loss=0.9498]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/50 - 16.73s\n",
      "Train Loss: 0.0976 (Mark: 0.0000, Time: 0.9761)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 198\u001b[0m\n\u001b[1;32m    195\u001b[0m x_sample, y_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    196\u001b[0m x_sample \u001b[38;5;241m=\u001b[39m x_sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m time_pred, mark_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m predicted_mark \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(mark_logits\u001b[38;5;241m.\u001b[39msqueeze(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m最终预测示例:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 38\u001b[0m, in \u001b[0;36mSimpleGRU.forward\u001b[0;34m(self, event_sequence)\u001b[0m\n\u001b[1;32m     35\u001b[0m marks \u001b[38;5;241m=\u001b[39m event_sequence[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     36\u001b[0m times \u001b[38;5;241m=\u001b[39m event_sequence[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m mark_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m event_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_gru(mark_embedded)\n\u001b[1;32m     40\u001b[0m time_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_gru(times)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) >= 2:  # 确保序列至少有2个事件\n",
    "                # 将序列转换为tensor\n",
    "                seq_tensor = torch.tensor([[event[0], event[1]] for event in seq], dtype=torch.float32)\n",
    "                # 分离输入序列(x)和目标值(y)\n",
    "                x = seq_tensor[:-1]  # 除最后一个外的所有事件\n",
    "                y = seq_tensor[-1]   # 最后一个事件\n",
    "                self.sequences.append((x, y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size_event, hidden_size_time, reg=0.1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg = reg\n",
    "        self.event_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        self.event_gru = nn.GRU(num_classes, hidden_size_event, batch_first=True)\n",
    "        self.time_gru = nn.GRU(1, hidden_size_time, batch_first=True)\n",
    "        combined_size = hidden_size_event + hidden_size_time\n",
    "        self.time_output = nn.Linear(combined_size, 1)\n",
    "        self.mark_output = nn.Linear(combined_size, num_classes)\n",
    "\n",
    "    def forward(self, event_sequence):\n",
    "        marks = event_sequence[..., 0].long()\n",
    "        times = event_sequence[..., 1].unsqueeze(-1)\n",
    "        \n",
    "        mark_embedded = self.event_embedding(marks)\n",
    "        event_output, _ = self.event_gru(mark_embedded)\n",
    "        time_output, _ = self.time_gru(times)\n",
    "        \n",
    "        combined_output = torch.cat([\n",
    "            event_output[:, -1, :],\n",
    "            time_output[:, -1, :]\n",
    "        ], dim=-1)\n",
    "        \n",
    "        time_pred = self.time_output(combined_output).unsqueeze(1)\n",
    "        mark_logits = self.mark_output(combined_output).unsqueeze(1)\n",
    "        return time_pred, mark_logits\n",
    "\n",
    "    def compute_loss(self, time_pred, mark_logits, targets):\n",
    "        true_times = targets[..., 1].unsqueeze(-1)\n",
    "        true_marks = targets[..., 0].long()\n",
    "        \n",
    "        time_loss = F.mse_loss(time_pred, true_times)\n",
    "        mark_logits_flat = mark_logits.squeeze(1)  # 移除多余的维度\n",
    "        mark_loss = F.cross_entropy(mark_logits_flat, true_marks)\n",
    "        \n",
    "        total_loss = mark_loss + self.reg * time_loss\n",
    "        return total_loss, mark_loss, time_loss\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    print(f\"Training on {device}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        train_mark_loss_sum = 0\n",
    "        train_time_loss_sum = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            batch_mark_loss = 0\n",
    "            batch_time_loss = 0\n",
    "            \n",
    "            for x, y in batch:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                x = x.unsqueeze(0)  # 添加batch维度\n",
    "                y = y.unsqueeze(0)  # 添加batch维度\n",
    "                \n",
    "                # 前向传播\n",
    "                time_pred, mark_logits = model(x)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss, mark_loss, time_loss = model.compute_loss(time_pred, mark_logits, y)\n",
    "                \n",
    "                batch_loss += loss\n",
    "                batch_mark_loss += mark_loss\n",
    "                batch_time_loss += time_loss\n",
    "            \n",
    "            batch_size = len(batch)\n",
    "            batch_loss /= batch_size\n",
    "            batch_mark_loss /= batch_size\n",
    "            batch_time_loss /= batch_size\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_total_loss += batch_loss.item()\n",
    "            train_mark_loss_sum += batch_mark_loss.item()\n",
    "            train_time_loss_sum += batch_time_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{batch_loss.item():.4f}',\n",
    "                'mark_loss': f'{batch_mark_loss.item():.4f}',\n",
    "                'time_loss': f'{batch_time_loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_total_loss / num_batches\n",
    "        avg_train_mark_loss = train_mark_loss_sum / num_batches\n",
    "        avg_train_time_loss = train_time_loss_sum / num_batches\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Mark: {avg_train_mark_loss:.4f}, Time: {avg_train_time_loss:.4f})')\n",
    "        \n",
    "        # 每5个epoch打印预测示例\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x_sample, y_sample = next(iter(train_loader))[0]\n",
    "                x_sample = x_sample.to(device)\n",
    "                x_sample = x_sample.unsqueeze(0)\n",
    "                \n",
    "                time_pred, mark_logits = model(x_sample)\n",
    "                predicted_mark = torch.argmax(mark_logits.squeeze(), dim=-1)\n",
    "                \n",
    "                print(\"\\n预测示例:\")\n",
    "                print(f\"实际标记: {y_sample[0].item()}\")\n",
    "                print(f\"预测标记: {predicted_mark.item()}\")\n",
    "                print(f\"标记概率分布: {torch.softmax(mark_logits.squeeze(), dim=-1)}\")\n",
    "            model.train()\n",
    "        \n",
    "        print('-' * 80)\n",
    "\n",
    "# 生成数据\n",
    "DIM_SIZE = 1\n",
    "BATCH_SIZE = 256\n",
    "mi = MarkedIntensityHomogenuosPoisson(DIM_SIZE)\n",
    "for u in range(DIM_SIZE):\n",
    "    mi.initialize(1.0, u)\n",
    "simulated_sequences = generate_samples_marked(mi, 100.0, 10000)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = PointProcessDataset(simulated_sequences)\n",
    "\n",
    "# 检查数据分布\n",
    "mark_counts = torch.zeros(DIM_SIZE)\n",
    "for x, y in dataset:\n",
    "    mark_counts[int(y[0])] += 1\n",
    "\n",
    "print(\"\\n标记分布:\")\n",
    "for i in range(DIM_SIZE):\n",
    "    print(f\"标记 {i}: {mark_counts[i]} 个样本 ({mark_counts[i]/len(dataset)*100:.2f}%)\")\n",
    "\n",
    "# 创建数据加载器\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 创建和训练模型\n",
    "model = SimpleGRU(\n",
    "    num_classes=DIM_SIZE,\n",
    "    hidden_size_event=64,\n",
    "    hidden_size_time=64,\n",
    "    reg=0.1\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=dataloader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'simple_gru_model.pth')\n",
    "\n",
    "# 测试预测\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_sample, y_sample = next(iter(dataloader))[0]\n",
    "    x_sample = x_sample.unsqueeze(0)\n",
    "    \n",
    "    time_pred, mark_logits = model(x_sample)\n",
    "    predicted_mark = torch.argmax(mark_logits.squeeze(), dim=-1)\n",
    "    \n",
    "    print(\"\\n最终预测示例:\")\n",
    "    print(f\"实际值 - 标记: {y_sample[0].item()}, 时间: {y_sample[1].item():.4f}\")\n",
    "    print(f\"预测值 - 标记: {predicted_mark.item()}, 时间: {time_pred.squeeze().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PointProcessDataset\n\u001b[1;32m      3\u001b[0m df_test\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/lia/scratch/wenqliu/ee556-2/master-project/point_process/mu_0.9_alpha_0.8_beta_1.0_T_1000_cluster.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m PointProcessDataset(T_test, time_step)\n",
      "File \u001b[0;32m/mnt/lia/scratch/wenqliu/ee556-2/Recurrent-Point-Process/model.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from model import PointProcessDataset\n",
    "df_test=pd.read_csv(\"/mnt/lia/scratch/wenqliu/ee556-2/master-project/point_process/mu_0.9_alpha_0.8_beta_1.0_T_1000_cluster.csv\")\n",
    "test_dataset = PointProcessDataset(T_test, time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 797.1 MB 21 kB/s s eta 0:00:01     |██████▊                         | 166.2 MB 41.6 MB/s eta 0:00:16     |███████▋                        | 188.3 MB 18.9 MB/s eta 0:00:33     |████████                        | 201.9 MB 33.9 MB/s eta 0:00:18     |█████████                       | 221.6 MB 33.9 MB/s eta 0:00:17     |█████████████▏                  | 327.0 MB 26.9 MB/s eta 0:00:18     |████████████████▌               | 411.9 MB 35.1 MB/s eta 0:00:11     |██████████████████▎             | 455.6 MB 39.1 MB/s eta 0:00:09     |███████████████████████▎        | 579.9 MB 70.4 MB/s eta 0:00:04     |███████████████████████▊        | 591.7 MB 70.4 MB/s eta 0:00:03     |████████████████████████        | 598.0 MB 17.8 MB/s eta 0:00:12     |█████████████████████████▉      | 642.9 MB 53.6 MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.1 MB 50.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 27.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /home/wenqliu/.local/lib/python3.8/site-packages (from torch) (4.12.2)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 410.6 MB 15 kB/s s eta 0:00:011   |███████                         | 88.9 MB 26.2 MB/s eta 0:00:13     |█████████████████▋              | 225.6 MB 55.6 MB/s eta 0:00:04     |█████████████████▊              | 227.9 MB 55.6 MB/s eta 0:00:04     |███████████████████▊            | 253.5 MB 45.9 MB/s eta 0:00:04     |███████████████████████████████▏| 399.7 MB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 124.2 MB 64.8 MB/s eta 0:00:01 eta 0:00:02     |█████████▏                      | 35.4 MB 87.0 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 56.5 MB 37.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 74.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.7 MB 71.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[K     |████████████████████████████████| 823 kB 88.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 86.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 96.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 664.8 MB 16 kB/s /s eta 0:00:01    |███████▊                        | 161.4 MB 20.7 MB/s eta 0:00:25     |█████████▉                      | 204.8 MB 69.6 MB/s eta 0:00:07     |███████████▊                    | 244.4 MB 69.6 MB/s eta 0:00:07     |█████████████████▊              | 368.9 MB 83.9 MB/s eta 0:00:04MB/s eta 0:00:02█████████     | 562.7 MB 84.0 MB/s eta 0:00:02██████████████████▊   | 596.1 MB 84.0 MB/s eta 0:00:01██████████████████████▊   | 597.0 MB 84.0 MB/s eta 0:00:01██████▉ | 640.7 MB 139.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 176.2 MB 61.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 196.0 MB 100 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\"\n",
      "  Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 209.4 MB 107 kB/s  eta 0:00:011��██████▊               | 109.1 MB 120.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 121.6 MB 147 kB/s  eta 0:00:01:022\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "\u001b[K     |████████████████████████████████| 179 kB 24.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.7 MB 108.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 92.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
      "Installing collected packages: filelock, nvidia-cuda-cupti-cu12, nvidia-nvtx-cu12, nvidia-cublas-cu12, nvidia-nvjitlink-cu12, nvidia-cusparse-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, networkx, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, mpmath, sympy, MarkupSafe, jinja2, nvidia-cudnn-cu12, nvidia-nccl-cu12, triton, nvidia-cufft-cu12, fsspec, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.16.1 fsspec-2024.10.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.1.105 sympy-1.13.3 torch-2.4.1 triton-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
