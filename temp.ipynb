{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from generation import MarkedIntensityHomogenuosPoisson, generate_samples_marked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 123,
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = [\n",
    "            torch.tensor([[event[0], event[1]] for event in seq], dtype=torch.float32)\n",
    "            for seq in sequences\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "DIM_SIZE = 7\n",
    "BATCH_SIZE=256\n",
    "mi = MarkedIntensityHomogenuosPoisson(DIM_SIZE)\n",
    "for u in range(DIM_SIZE):\n",
    "    mi.initialize(1.0, u)\n",
    "simulated_sequences = generate_samples_marked(mi, 20.0, 10000)\n",
    "dataset = PointProcessDataset(simulated_sequences)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 124,
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size_event, hidden_size_time, reg=0.1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg = reg\n",
    "        self.event_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        self.event_gru = nn.GRU(num_classes, hidden_size_event, batch_first=True)\n",
    "        self.time_gru = nn.GRU(1, hidden_size_time, batch_first=True)\n",
    "        combined_size = hidden_size_event + hidden_size_time\n",
    "        self.time_output = nn.Linear(combined_size, 1)\n",
    "        self.mark_output = nn.Linear(combined_size, num_classes)\n",
    "\n",
    "    def forward(self, event_sequence):\n",
    "        marks = event_sequence[..., 0].long()\n",
    "        times = event_sequence[..., 1].unsqueeze(-1)\n",
    "        mark_embedded = self.event_embedding(marks)\n",
    "        event_output, _ = self.event_gru(mark_embedded)\n",
    "        time_output, _ = self.time_gru(times)\n",
    "        combined_output = torch.cat([event_output, time_output], dim=-1)\n",
    "        time_pred = self.time_output(combined_output)\n",
    "        mark_logits = self.mark_output(combined_output)\n",
    "        return time_pred, mark_logits\n",
    "    def compute_loss(self, time_pred, mark_logits, targets):\n",
    "        true_times = targets[..., 1].unsqueeze(-1)\n",
    "        true_marks = targets[..., 0].long()\n",
    "        time_loss = F.mse_loss(time_pred, true_times)\n",
    "        mark_logits_flat = mark_logits.view(-1, self.num_classes)\n",
    "        true_marks_flat = true_marks.view(-1)\n",
    "        mark_loss = F.cross_entropy(mark_logits_flat, true_marks_flat)\n",
    "        total_loss = mark_loss + self.reg * time_loss\n",
    "        return total_loss, mark_loss, time_loss\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 126,
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1/100: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s, loss=11.5613, mark_loss=1.8686, time_loss=96.9274] \n"
=======
      "Epoch 1/10:   0%|          | 0/4 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 75\u001b[0m\n\u001b[1;32m     66\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleGRU(\n\u001b[1;32m     67\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mDIM_SIZE,\n\u001b[1;32m     68\u001b[0m     hidden_size_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     69\u001b[0m     hidden_size_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# 创建训练数据加载\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[126], line 44\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m batch_mark_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     43\u001b[0m batch_time_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m---> 44\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     46\u001b[0m train_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    训练模型的函数\n",
    "    \n",
    "    Args:\n",
    "        model: SimpleGRU模型实例\n",
    "        train_loader: 训练数据的DataLoader\n",
    "        num_epochs: 训练轮数\n",
    "        learning_rate: 学习率\n",
    "        device: 训练设备（'cuda'或'cpu'）\n",
    "    \"\"\"\n",
    "    print(f\"Training on {device}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        train_mark_loss_sum = 0\n",
    "        train_time_loss_sum = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            batch_mark_loss = 0\n",
    "            batch_time_loss = 0\n",
    "            \n",
    "            for sequence in batch:\n",
    "                sequence = sequence.to(device)\n",
    "                sequence = sequence.unsqueeze(0)\n",
    "                time_pred, mark_logits = model(sequence)\n",
    "                loss, mark_loss, time_loss = model.compute_loss(time_pred, mark_logits, sequence)\n",
    "                batch_loss += loss\n",
    "                batch_mark_loss += mark_loss\n",
    "                batch_time_loss += time_loss\n",
    "            batch_size = len(batch)\n",
    "            batch_loss /= batch_size\n",
    "            batch_mark_loss /= batch_size\n",
    "            batch_time_loss /= batch_size\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_total_loss += batch_loss.item()\n",
    "            train_mark_loss_sum += batch_mark_loss.item()\n",
    "            train_time_loss_sum += batch_time_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{batch_loss.item():.4f}',\n",
    "                'mark_loss': f'{batch_mark_loss.item():.4f}',\n",
    "                'time_loss': f'{batch_time_loss.item():.4f}'\n",
    "            })\n",
    "        avg_train_loss = train_total_loss / num_batches\n",
    "        avg_train_mark_loss = train_mark_loss_sum / num_batches\n",
    "        avg_train_time_loss = train_time_loss_sum / num_batches\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Mark: {avg_train_mark_loss:.4f}, Time: {avg_train_time_loss:.4f})')\n",
    "        print('-' * 80)\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建模型\n",
    "    model = SimpleGRU(\n",
    "        num_classes=DIM_SIZE,\n",
    "        hidden_size_event=16,\n",
    "        hidden_size_time=32\n",
    "    )\n",
    "    \n",
    "    # 创建训练数据加载\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=dataloader,\n",
    "        num_epochs=10,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "marked_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "################################### for neural network modeling\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenqing_liu/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 4999/4999 [00:15<00:00, 314.21it/s, loss=1.4620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Average Train Loss: 1.0540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 4999/4999 [00:15<00:00, 327.60it/s, loss=1.2577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20\n",
      "Average Train Loss: 1.0088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 4999/4999 [00:15<00:00, 332.41it/s, loss=1.4425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20\n",
      "Average Train Loss: 1.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 4999/4999 [00:15<00:00, 315.38it/s, loss=1.5970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20\n",
      "Average Train Loss: 1.0075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|██████████| 4999/4999 [00:16<00:00, 306.56it/s, loss=1.0575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20\n",
      "Average Train Loss: 1.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|██████████| 4999/4999 [00:17<00:00, 280.02it/s, loss=0.8609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20\n",
      "Average Train Loss: 1.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|██████████| 4999/4999 [00:17<00:00, 290.16it/s, loss=1.6346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20\n",
      "Average Train Loss: 1.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]:  77%|███████▋  | 3832/4999 [00:13<00:04, 283.86it/s, loss=0.8539]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 217\u001b[0m\n\u001b[1;32m    209\u001b[0m model \u001b[38;5;241m=\u001b[39m RNNPointProcess(\n\u001b[1;32m    210\u001b[0m     time_step\u001b[38;5;241m=\u001b[39mtime_step,\n\u001b[1;32m    211\u001b[0m     size_rnn\u001b[38;5;241m=\u001b[39msize_rnn,\n\u001b[1;32m    212\u001b[0m     size_nn\u001b[38;5;241m=\u001b[39msize_nn,\n\u001b[1;32m    213\u001b[0m     size_layer_chfn\u001b[38;5;241m=\u001b[39msize_layer_chfn\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[1;32m    225\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrnn_point_process.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[133], line 163\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# 检查loss是否为nan\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss):\n\u001b[0;32m--> 163\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# 添加梯度裁剪\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 生成训练和测试数据\n",
    "T_train = np.random.exponential(size=80000).cumsum()\n",
    "T_test = np.random.exponential(size=20000).cumsum()\n",
    "\n",
    "# 计算标准化参数\n",
    "mu = np.log(np.ediff1d(T_train)).mean()\n",
    "sigma = np.log(np.ediff1d(T_train)).std()\n",
    "\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, times, time_step):\n",
    "        self.times = times\n",
    "        self.time_step = time_step\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.times) - self.time_step - 1\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # 获取历史事件序列\n",
    "        history = self.times[idx:idx+self.time_step]\n",
    "        history = np.diff(history)  # 计算事件间隔\n",
    "        history = history.reshape(-1, 1)\n",
    "        \n",
    "        # 获取经过的时间\n",
    "        elapsed = self.times[idx+self.time_step+1] - self.times[idx+self.time_step]\n",
    "        \n",
    "        return {\n",
    "            'history': torch.FloatTensor(history),\n",
    "            'elapsed': torch.FloatTensor([elapsed]).clone()\n",
    "        }\n",
    "class RNNPointProcess(nn.Module):\n",
    "    def __init__(self, time_step=20, size_rnn=64, size_nn=64, size_layer_chfn=2):\n",
    "        super(RNNPointProcess, self).__init__()\n",
    "        self.time_step = time_step\n",
    "        self.size_rnn = size_rnn\n",
    "        self.size_nn = size_nn\n",
    "        \n",
    "        # RNN层\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size=size_rnn,\n",
    "            batch_first=True,\n",
    "            nonlinearity='tanh'  # 明确指定非线性函数\n",
    "        )\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        self.elapsed_time_linear = nn.Linear(1, size_nn, bias=False)\n",
    "        self.rnn_linear = nn.Linear(size_rnn, size_nn)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(size_nn, size_nn)\n",
    "            for _ in range(size_layer_chfn-1)\n",
    "        ])\n",
    "        \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(size_nn, 1)\n",
    "        \n",
    "        # 初始化为正权重\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # 注册标准化参数\n",
    "        self.register_buffer('mu', torch.tensor(mu, dtype=torch.float32))\n",
    "        self.register_buffer('sigma', torch.tensor(sigma, dtype=torch.float32))\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 使用较小的初始值\n",
    "            nn.init.uniform_(module.weight, 0, 0.1)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, event_history, elapsed_time):\n",
    "        # 添加数值稳定性检查\n",
    "        event_history = torch.clamp(event_history, min=1e-6)\n",
    "        elapsed_time = torch.clamp(elapsed_time, min=1e-6)\n",
    "        elapsed_time = elapsed_time.requires_grad_(True)\n",
    "        \n",
    "        # 标准化输入\n",
    "        event_history_norm = (torch.log(event_history) - self.mu) / self.sigma\n",
    "        elapsed_time_norm = (torch.log(elapsed_time) - self.mu) / self.sigma\n",
    "        \n",
    "        # 检查并处理无效值\n",
    "        event_history_norm = torch.nan_to_num(event_history_norm, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        elapsed_time_norm = torch.nan_to_num(elapsed_time_norm, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        # RNN处理历史序列\n",
    "        rnn_out, _ = self.rnn(event_history_norm)\n",
    "        rnn_out = rnn_out[:, -1, :]\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        hidden_tau = self.elapsed_time_linear(elapsed_time_norm)\n",
    "        hidden_rnn = self.rnn_linear(rnn_out)\n",
    "        hidden = torch.tanh(hidden_tau + hidden_rnn)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = torch.tanh(layer(hidden))\n",
    "        \n",
    "        # 计算累积危险函数（添加数值稳定性）\n",
    "        Int_l = F.softplus(self.output_layer(hidden)) + 1e-6\n",
    "        \n",
    "        # 计算危险函数\n",
    "        try:\n",
    "            l = torch.autograd.grad(Int_l.sum(), elapsed_time, create_graph=True)[0]\n",
    "            # 确保l是正的且有界\n",
    "            l = torch.clamp(l, min=1e-6, max=1e6)\n",
    "        except RuntimeError:\n",
    "            print(\"Gradient computation failed\")\n",
    "            l = torch.ones_like(elapsed_time) * 1e-6\n",
    "        \n",
    "        return l, Int_l\n",
    "    \n",
    "    def compute_loss(self, l, Int_l):\n",
    "        # 添加数值稳定性\n",
    "        l = torch.clamp(l, min=1e-6)\n",
    "        Int_l = torch.clamp(Int_l, min=0.0)\n",
    "        \n",
    "        # 计算负对数似然，添加梯度裁剪\n",
    "        loss = -torch.mean(torch.log(l) - Int_l)\n",
    "        \n",
    "        # 检查loss是否为nan\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Warning: Loss is NaN!\")\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=l.device)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "def train_model(model, train_loader, test_loader=None, num_epochs=10, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # 添加学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in train_bar:\n",
    "            history = batch['history'].to(device)\n",
    "            elapsed = batch['elapsed'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            l, Int_l = model(history, elapsed)\n",
    "            loss = model.compute_loss(l, Int_l)\n",
    "            \n",
    "            # 检查loss是否为nan\n",
    "            if not torch.isnan(loss):\n",
    "                loss.backward()\n",
    "                # 添加梯度裁剪\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Average Train Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(avg_train_loss)\n",
    "\n",
    "# 修改超参数\n",
    "if __name__ == \"__main__\":\n",
    "    time_step = 20\n",
    "    size_rnn = 32  # 减小RNN大小\n",
    "    size_nn = 32   # 减小隐藏层大小\n",
    "    size_layer_chfn = 2\n",
    "    batch_size = 16  # 减小批量大小\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.0001  # 使用更小的学习率\n",
    "    \n",
    "    # 其余代码保持不变...\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = PointProcessDataset(T_train, time_step)\n",
    "    test_dataset = PointProcessDataset(T_test, time_step)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    model = RNNPointProcess(\n",
    "        time_step=time_step,\n",
    "        size_rnn=size_rnn,\n",
    "        size_nn=size_nn,\n",
    "        size_layer_chfn=size_layer_chfn\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'rnn_point_process.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 186.17it/s, loss=1.7882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Average Train Loss: 2.4767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 200.08it/s, loss=1.2375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20\n",
      "Average Train Loss: 1.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 195.60it/s, loss=0.8734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20\n",
      "Average Train Loss: 1.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 195.06it/s, loss=1.0262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20\n",
      "Average Train Loss: 1.1403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]:  52%|█████▏    | 162/312 [00:00<00:00, 196.18it/s, loss=1.0139]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 218\u001b[0m\n\u001b[1;32m    210\u001b[0m model \u001b[38;5;241m=\u001b[39m GRUPointProcess(\n\u001b[1;32m    211\u001b[0m     time_step\u001b[38;5;241m=\u001b[39mtime_step,\n\u001b[1;32m    212\u001b[0m     size_gru\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    213\u001b[0m     size_nn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    214\u001b[0m     size_layer_chfn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    215\u001b[0m )\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\n\u001b[1;32m    224\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[135], line 160\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    158\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# 移除梯度裁剪\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    163\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/adam.py:367\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    364\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 367\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n\u001b[1;32m    370\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 数据集类\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, data, time_step=20):\n",
    "        self.data = data\n",
    "        self.time_step = time_step\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.time_step\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        history = self.data[idx:idx+self.time_step].reshape(-1, 1)\n",
    "        elapsed = self.data[idx+self.time_step].reshape(-1)\n",
    "        \n",
    "        return {\n",
    "            'history': torch.FloatTensor(history),\n",
    "            'elapsed': torch.FloatTensor(elapsed)\n",
    "        }\n",
    "\n",
    "class GRUPointProcess(nn.Module):\n",
    "    def __init__(self, time_step=20, size_gru=64, size_nn=64, size_layer_chfn=2):\n",
    "        super(GRUPointProcess, self).__init__()\n",
    "        self.time_step = time_step\n",
    "        self.size_gru = size_gru\n",
    "        self.size_nn = size_nn\n",
    "        \n",
    "        # GRU层\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=size_gru,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        self.elapsed_time_linear = nn.Linear(1, size_nn, bias=False)\n",
    "        self.gru_linear = nn.Linear(size_gru, size_nn)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(size_nn, size_nn)\n",
    "            for _ in range(size_layer_chfn-1)\n",
    "        ])\n",
    "        \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(size_nn, 1)\n",
    "        \n",
    "        # 初始化为正权重\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # 注册标准化参数\n",
    "        self.register_buffer('mu', torch.tensor(mu, dtype=torch.float32))\n",
    "        self.register_buffer('sigma', torch.tensor(sigma, dtype=torch.float32))\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 使用较小的初始值\n",
    "            nn.init.uniform_(module.weight, 0, 0.1)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, event_history, elapsed_time):\n",
    "        # 添加数值稳定性检查\n",
    "        event_history = torch.clamp(event_history, min=1e-6)\n",
    "        elapsed_time = torch.clamp(elapsed_time, min=1e-6)\n",
    "        elapsed_time = elapsed_time.requires_grad_(True)\n",
    "        \n",
    "        # 标准化输入\n",
    "        event_history_norm = (torch.log(event_history) - self.mu) / self.sigma\n",
    "        elapsed_time_norm = (torch.log(elapsed_time) - self.mu) / self.sigma\n",
    "        \n",
    "        # 检查并处理无效值\n",
    "        event_history_norm = torch.nan_to_num(event_history_norm, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        elapsed_time_norm = torch.nan_to_num(elapsed_time_norm, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        # GRU处理历史序列\n",
    "        gru_out, _ = self.gru(event_history_norm)\n",
    "        gru_out = gru_out[:, -1, :]  # 取最后一个时间步的输出\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        hidden_tau = self.elapsed_time_linear(elapsed_time_norm)\n",
    "        hidden_gru = self.gru_linear(gru_out)\n",
    "        hidden = torch.tanh(hidden_tau + hidden_gru)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = torch.tanh(layer(hidden))\n",
    "        \n",
    "        # 计算累积危险函数（添加数值稳定性）\n",
    "        Int_l = F.softplus(self.output_layer(hidden)) + 1e-6\n",
    "        \n",
    "        # 计算危险函数\n",
    "        try:\n",
    "            l = torch.autograd.grad(Int_l.sum(), elapsed_time, create_graph=True)[0]\n",
    "            # 确保l是正的且有界\n",
    "            l = torch.clamp(l, min=1e-6, max=1e6)\n",
    "        except RuntimeError:\n",
    "            print(\"Gradient computation failed\")\n",
    "            l = torch.ones_like(elapsed_time) * 1e-6\n",
    "        \n",
    "        return l, Int_l\n",
    "    \n",
    "    def compute_loss(self, l, Int_l):\n",
    "        # 添加数值稳定性\n",
    "        l = torch.clamp(l, min=1e-6)\n",
    "        Int_l = torch.clamp(Int_l, min=0.0)\n",
    "        \n",
    "        # 计算负对数似然，添加梯度裁剪\n",
    "        loss = -torch.mean(torch.log(l) - Int_l)\n",
    "        \n",
    "        # 检查loss是否为nan\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Warning: Loss is NaN!\")\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=l.device)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "def train_model(model, train_loader, test_loader=None, num_epochs=10, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # 添加学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in train_bar:\n",
    "            history = batch['history'].to(device)\n",
    "            elapsed = batch['elapsed'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            l, Int_l = model(history, elapsed)\n",
    "            loss = model.compute_loss(l, Int_l)\n",
    "            \n",
    "            # 检查loss是否为nan\n",
    "            if not torch.isnan(loss):\n",
    "                loss.backward()\n",
    "                # 移除梯度裁剪\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Average Train Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(avg_train_loss)\n",
    "        \n",
    "        # 保存模型\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f'gru_point_process_epoch_{epoch+1}.pt')\n",
    "# 生成训练数据\n",
    "def generate_data(num_samples, lambda_=1.0):\n",
    "    return np.random.exponential(1/lambda_, num_samples)\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成数据\n",
    "    num_samples = 10000\n",
    "    T_train = generate_data(num_samples)\n",
    "    T_test = generate_data(num_samples // 10)\n",
    "    \n",
    "    # 计算标准化参数\n",
    "    mu = np.mean(np.log(T_train))\n",
    "    sigma = np.std(np.log(T_train))\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    time_step = 20\n",
    "    batch_size = 32\n",
    "    train_dataset = PointProcessDataset(T_train, time_step)\n",
    "    test_dataset = PointProcessDataset(T_test, time_step)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    model = GRUPointProcess(\n",
    "        time_step=time_step,\n",
    "        size_gru=32,\n",
    "        size_nn=32,\n",
    "        size_layer_chfn=2\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        num_epochs=20,\n",
    "        learning_rate=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenqing_liu/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 182.09it/s, loss=2.1096]\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Epoch 1/100 - 15.80s\n",
      "Train Loss: 13.6339 (Mark: 1.9144, Time: 117.1947)\n",
      "--------------------------------------------------------------------------------\n"
=======
      "Epoch 1/20\n",
      "Average Train Loss: 2.0911\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 2/100: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s, loss=7.9704, mark_loss=1.7912, time_loss=61.7925] \n"
=======
      "Epoch 2/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 187.04it/s, loss=1.8564]\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Epoch 2/100 - 15.61s\n",
      "Train Loss: 9.4751 (Mark: 1.8251, Time: 76.4998)\n",
      "--------------------------------------------------------------------------------\n"
=======
      "Epoch 2/20\n",
      "Average Train Loss: 1.8369\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 3/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=5.6050, mark_loss=1.7466, time_loss=38.5845]\n"
=======
      "Epoch 3/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 205.60it/s, loss=1.8494]\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Epoch 3/100 - 15.92s\n",
      "Train Loss: 6.5905 (Mark: 1.7791, Time: 48.1141)\n",
      "--------------------------------------------------------------------------------\n"
=======
      "Epoch 3/20\n",
      "Average Train Loss: 1.7539\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 4/100: 100%|██████████| 40/40 [00:15<00:00,  2.54it/s, loss=4.2511, mark_loss=1.5640, time_loss=26.8707]\n"
=======
      "Epoch 4/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 199.71it/s, loss=1.8517]\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Epoch 4/100 - 15.73s\n",
      "Train Loss: 4.8883 (Mark: 1.6622, Time: 32.2607)\n",
      "--------------------------------------------------------------------------------\n"
=======
      "Epoch 4/20\n",
      "Average Train Loss: 1.7271\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 5/100: 100%|██████████| 40/40 [00:16<00:00,  2.48it/s, loss=3.3359, mark_loss=1.3476, time_loss=19.8830]\n"
=======
      "Epoch 5/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 202.49it/s, loss=1.8450]\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Epoch 5/100 - 16.16s\n",
      "Train Loss: 3.7368 (Mark: 1.4582, Time: 22.7860)\n",
      "--------------------------------------------------------------------------------\n"
=======
      "Epoch 5/20\n",
      "Average Train Loss: 1.7144\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 6/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=2.5397, mark_loss=1.1251, time_loss=14.1461]\n"
=======
      "Epoch 6/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 200.63it/s, loss=1.8373]\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Epoch 6/100 - 15.97s\n",
      "Train Loss: 2.8590 (Mark: 1.2296, Time: 16.2939)\n",
      "--------------------------------------------------------------------------------\n"
=======
      "Epoch 6/20\n",
      "Average Train Loss: 1.7057\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 7/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=1.9301, mark_loss=0.9410, time_loss=9.8906] \n"
=======
      "Epoch 7/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 202.44it/s, loss=1.8311]\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Epoch 7/100 - 15.89s\n",
      "Train Loss: 2.2090 (Mark: 1.0228, Time: 11.8621)\n",
      "--------------------------------------------------------------------------------\n"
=======
      "Epoch 7/20\n",
      "Average Train Loss: 1.7007\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 8/100: 100%|██████████| 40/40 [00:15<00:00,  2.54it/s, loss=1.5241, mark_loss=0.7967, time_loss=7.2737]\n"
=======
      "Epoch 8/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 199.97it/s, loss=1.8264]\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Epoch 8/100 - 15.75s\n",
      "Train Loss: 1.7347 (Mark: 0.8618, Time: 8.7290)\n",
      "--------------------------------------------------------------------------------\n"
=======
      "Epoch 8/20\n",
      "Average Train Loss: 1.6973\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 9/100: 100%|██████████| 40/40 [00:16<00:00,  2.45it/s, loss=1.2360, mark_loss=0.6854, time_loss=5.5068]\n"
=======
      "Epoch 9/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 203.79it/s, loss=1.8234]\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Epoch 9/100 - 16.33s\n",
      "Train Loss: 1.3904 (Mark: 0.7398, Time: 6.5054)\n",
      "--------------------------------------------------------------------------------\n"
=======
      "Epoch 9/20\n",
      "Average Train Loss: 1.6951\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 10/100: 100%|██████████| 40/40 [00:16<00:00,  2.49it/s, loss=1.0310, mark_loss=0.6072, time_loss=4.2375]\n"
=======
      "Epoch 10/20 [Train]: 100%|██████████| 312/312 [00:01<00:00, 195.05it/s, loss=1.8211]\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Epoch 10/100 - 16.04s\n",
      "Train Loss: 1.1363 (Mark: 0.6425, Time: 4.9382)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 40/40 [00:16<00:00,  2.49it/s, loss=0.8442, mark_loss=0.5227, time_loss=3.2154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/100 - 16.09s\n",
      "Train Loss: 0.9419 (Mark: 0.5572, Time: 3.8467)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 40/40 [00:16<00:00,  2.47it/s, loss=0.7181, mark_loss=0.4271, time_loss=2.9100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/100 - 16.19s\n",
      "Train Loss: 0.7787 (Mark: 0.4695, Time: 3.0923)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=0.5950, mark_loss=0.3657, time_loss=2.2934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/100 - 15.92s\n",
      "Train Loss: 0.6402 (Mark: 0.3927, Time: 2.4746)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 40/40 [00:15<00:00,  2.50it/s, loss=0.4894, mark_loss=0.3148, time_loss=1.7459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/100 - 15.98s\n",
      "Train Loss: 0.5318 (Mark: 0.3340, Time: 1.9779)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s, loss=0.4183, mark_loss=0.2711, time_loss=1.4716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/100 - 15.65s\n",
      "Train Loss: 0.4474 (Mark: 0.2889, Time: 1.5858)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 40/40 [00:15<00:00,  2.58it/s, loss=0.3489, mark_loss=0.2338, time_loss=1.1511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/100 - 15.53s\n",
      "Train Loss: 0.3790 (Mark: 0.2510, Time: 1.2800)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.2959, mark_loss=0.2052, time_loss=0.9076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/100 - 15.84s\n",
      "Train Loss: 0.3235 (Mark: 0.2188, Time: 1.0464)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 40/40 [00:16<00:00,  2.49it/s, loss=0.2581, mark_loss=0.1776, time_loss=0.8055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/100 - 16.09s\n",
      "Train Loss: 0.2788 (Mark: 0.1917, Time: 0.8705)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 40/40 [00:16<00:00,  2.46it/s, loss=0.2241, mark_loss=0.1586, time_loss=0.6549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/100 - 16.23s\n",
      "Train Loss: 0.2426 (Mark: 0.1692, Time: 0.7336)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 40/40 [00:16<00:00,  2.43it/s, loss=0.2010, mark_loss=0.1413, time_loss=0.5966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/100 - 16.47s\n",
      "Train Loss: 0.2128 (Mark: 0.1502, Time: 0.6259)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=0.1664, mark_loss=0.1219, time_loss=0.4446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/100 - 17.21s\n",
      "Train Loss: 0.1852 (Mark: 0.1322, Time: 0.5298)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s, loss=0.1434, mark_loss=0.1043, time_loss=0.3911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/100 - 15.78s\n",
      "Train Loss: 0.1577 (Mark: 0.1132, Time: 0.4451)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s, loss=0.1223, mark_loss=0.0890, time_loss=0.3327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/100 - 15.82s\n",
      "Train Loss: 0.1335 (Mark: 0.0961, Time: 0.3736)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 40/40 [00:16<00:00,  2.49it/s, loss=0.1089, mark_loss=0.0761, time_loss=0.3278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/100 - 16.07s\n",
      "Train Loss: 0.1143 (Mark: 0.0824, Time: 0.3194)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 40/40 [00:16<00:00,  2.48it/s, loss=0.0940, mark_loss=0.0674, time_loss=0.2662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/100 - 16.12s\n",
      "Train Loss: 0.0994 (Mark: 0.0717, Time: 0.2775)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 40/40 [00:16<00:00,  2.44it/s, loss=0.0856, mark_loss=0.0604, time_loss=0.2523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/100 - 16.41s\n",
      "Train Loss: 0.0879 (Mark: 0.0636, Time: 0.2432)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 40/40 [00:15<00:00,  2.50it/s, loss=0.0742, mark_loss=0.0550, time_loss=0.1918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/100 - 15.99s\n",
      "Train Loss: 0.0784 (Mark: 0.0571, Time: 0.2131)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 40/40 [00:16<00:00,  2.47it/s, loss=0.0688, mark_loss=0.0498, time_loss=0.1905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/100 - 16.17s\n",
      "Train Loss: 0.0707 (Mark: 0.0517, Time: 0.1895)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 40/40 [00:16<00:00,  2.48it/s, loss=0.0610, mark_loss=0.0460, time_loss=0.1501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/100 - 16.11s\n",
      "Train Loss: 0.0642 (Mark: 0.0475, Time: 0.1675)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 40/40 [00:16<00:00,  2.46it/s, loss=0.0549, mark_loss=0.0428, time_loss=0.1205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/100 - 16.28s\n",
      "Train Loss: 0.0588 (Mark: 0.0440, Time: 0.1485)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.0517, mark_loss=0.0395, time_loss=0.1217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/100 - 15.85s\n",
      "Train Loss: 0.0542 (Mark: 0.0410, Time: 0.1328)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 40/40 [00:16<00:00,  2.45it/s, loss=0.0497, mark_loss=0.0377, time_loss=0.1197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/100 - 16.32s\n",
      "Train Loss: 0.0503 (Mark: 0.0383, Time: 0.1192)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|██████████| 40/40 [00:16<00:00,  2.44it/s, loss=0.0461, mark_loss=0.0348, time_loss=0.1135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/100 - 16.39s\n",
      "Train Loss: 0.0467 (Mark: 0.0360, Time: 0.1074)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.0430, mark_loss=0.0330, time_loss=0.0999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/100 - 15.87s\n",
      "Train Loss: 0.0436 (Mark: 0.0339, Time: 0.0969)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=0.0394, mark_loss=0.0312, time_loss=0.0820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/100 - 15.97s\n",
      "Train Loss: 0.0408 (Mark: 0.0320, Time: 0.0875)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|██████████| 40/40 [00:16<00:00,  2.50it/s, loss=0.0370, mark_loss=0.0297, time_loss=0.0722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/100 - 16.01s\n",
      "Train Loss: 0.0383 (Mark: 0.0303, Time: 0.0797)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|██████████| 40/40 [00:15<00:00,  2.54it/s, loss=0.0351, mark_loss=0.0279, time_loss=0.0726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/100 - 15.73s\n",
      "Train Loss: 0.0361 (Mark: 0.0288, Time: 0.0729)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s, loss=0.0322, mark_loss=0.0266, time_loss=0.0565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/100 - 15.61s\n",
      "Train Loss: 0.0340 (Mark: 0.0273, Time: 0.0665)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=0.0319, mark_loss=0.0253, time_loss=0.0664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/100 - 15.92s\n",
      "Train Loss: 0.0322 (Mark: 0.0260, Time: 0.0614)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.0301, mark_loss=0.0242, time_loss=0.0580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/100 - 15.88s\n",
      "Train Loss: 0.0304 (Mark: 0.0248, Time: 0.0563)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.0279, mark_loss=0.0230, time_loss=0.0486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/100 - 15.85s\n",
      "Train Loss: 0.0288 (Mark: 0.0237, Time: 0.0516)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=0.0263, mark_loss=0.0219, time_loss=0.0440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/100 - 15.93s\n",
      "Train Loss: 0.0274 (Mark: 0.0226, Time: 0.0475)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s, loss=0.0254, mark_loss=0.0214, time_loss=0.0403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/100 - 15.63s\n",
      "Train Loss: 0.0260 (Mark: 0.0216, Time: 0.0439)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|██████████| 40/40 [00:15<00:00,  2.52it/s, loss=0.0241, mark_loss=0.0203, time_loss=0.0382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/100 - 15.90s\n",
      "Train Loss: 0.0248 (Mark: 0.0207, Time: 0.0405)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s, loss=0.0234, mark_loss=0.0195, time_loss=0.0387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/100 - 15.92s\n",
      "Train Loss: 0.0236 (Mark: 0.0199, Time: 0.0375)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|██████████| 40/40 [00:16<00:00,  2.48it/s, loss=0.0222, mark_loss=0.0187, time_loss=0.0355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/100 - 16.12s\n",
      "Train Loss: 0.0225 (Mark: 0.0191, Time: 0.0346)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s, loss=0.0211, mark_loss=0.0182, time_loss=0.0293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/100 - 15.84s\n",
      "Train Loss: 0.0215 (Mark: 0.0183, Time: 0.0319)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|██████████| 40/40 [00:16<00:00,  2.48it/s, loss=0.0200, mark_loss=0.0174, time_loss=0.0261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/100 - 16.14s\n",
      "Train Loss: 0.0206 (Mark: 0.0176, Time: 0.0296)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100:  15%|█▌        | 6/40 [00:02<00:16,  2.11it/s, loss=0.0199, mark_loss=0.0172, time_loss=0.0276]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 77\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# 创建模型\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     model \u001b[38;5;241m=\u001b[39m SimpleGRU(\n\u001b[1;32m     72\u001b[0m         num_classes\u001b[38;5;241m=\u001b[39mDIM_SIZE,\n\u001b[1;32m     73\u001b[0m         hidden_size_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     74\u001b[0m         hidden_size_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[0;32m---> 77\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 46\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m batch_mark_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     45\u001b[0m batch_time_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m---> 46\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m train_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # 导入 tqdm 函数\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    训练模型的函数\n",
    "    \n",
    "    Args:\n",
    "        model: SimpleGRU模型实例\n",
    "        train_loader: 训练数据的DataLoader\n",
    "        num_epochs: 训练轮数\n",
    "        learning_rate: 学习率\n",
    "        device: 训练设备（'cuda'或'cpu'）\n",
    "    \"\"\"\n",
    "    print(f\"Training on {device}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        train_mark_loss_sum = 0\n",
    "        train_time_loss_sum = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            batch_mark_loss = 0\n",
    "            batch_time_loss = 0\n",
    "            \n",
    "            for sequence in batch:\n",
    "                sequence = sequence.to(device)\n",
    "                sequence = sequence.unsqueeze(0)\n",
    "                time_pred, mark_logits = model(sequence)\n",
    "                loss, mark_loss, time_loss = model.compute_loss(time_pred, mark_logits, sequence)\n",
    "                batch_loss += loss\n",
    "                batch_mark_loss += mark_loss\n",
    "                batch_time_loss += time_loss\n",
    "            batch_size = len(batch)\n",
    "            batch_loss /= batch_size\n",
    "            batch_mark_loss /= batch_size\n",
    "            batch_time_loss /= batch_size\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_total_loss += batch_loss.item()\n",
    "            train_mark_loss_sum += batch_mark_loss.item()\n",
    "            train_time_loss_sum += batch_time_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{batch_loss.item():.4f}',\n",
    "                'mark_loss': f'{batch_mark_loss.item():.4f}',\n",
    "                'time_loss': f'{batch_time_loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_total_loss / num_batches\n",
    "        avg_train_mark_loss = train_mark_loss_sum / num_batches\n",
    "        avg_train_time_loss = train_time_loss_sum / num_batches\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Mark: {avg_train_mark_loss:.4f}, Time: {avg_train_time_loss:.4f})')\n",
    "        print('-' * 80)\n",
    "\n",
    "# 使用示例：\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建模型\n",
    "    model = SimpleGRU(\n",
    "        num_classes=DIM_SIZE,\n",
    "        hidden_size_event=16,\n",
    "        hidden_size_time=32\n",
    "    )\n",
    "    \n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=dataloader,\n",
    "        num_epochs=100,\n",
    "        learning_rate=0.001\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) >= 2:  # 确保序列至少有2个事件\n",
    "                # 将序列转换为tensor\n",
    "                seq_tensor = torch.tensor([[event[0], event[1]] for event in seq], dtype=torch.float32)\n",
    "                # 分离输入序列(x)和目标值(y)\n",
    "                x = seq_tensor[:-1]  # 除最后一个外的所有事件\n",
    "                y = seq_tensor[-1]   # 最后一个事件\n",
    "                self.sequences.append((x, y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch中的每个元素都是(x, y)对\n",
    "    return batch\n",
    "\n",
    "# 生成数据\n",
    "DIM_SIZE = 7\n",
    "BATCH_SIZE = 256\n",
    "mi = MarkedIntensityHomogenuosPoisson(DIM_SIZE)\n",
    "for u in range(DIM_SIZE):\n",
    "    mi.initialize(1.0, u)\n",
    "simulated_sequences = generate_samples_marked(mi, 20.0, 10000)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = PointProcessDataset(simulated_sequences)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size_event, hidden_size_time, reg=0.1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg = reg\n",
    "        self.event_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        self.event_gru = nn.GRU(num_classes, hidden_size_event, batch_first=True)\n",
    "        self.time_gru = nn.GRU(1, hidden_size_time, batch_first=True)\n",
    "        combined_size = hidden_size_event + hidden_size_time\n",
    "        self.time_output = nn.Linear(combined_size, 1)\n",
    "        self.mark_output = nn.Linear(combined_size, num_classes)\n",
    "\n",
    "    def forward(self, event_sequence):\n",
    "        marks = event_sequence[..., 0].long()\n",
    "        times = event_sequence[..., 1].unsqueeze(-1)\n",
    "        \n",
    "        # 处理所有时间步\n",
    "        mark_embedded = self.event_embedding(marks)\n",
    "        event_output, _ = self.event_gru(mark_embedded)\n",
    "        time_output, _ = self.time_gru(times)\n",
    "        \n",
    "        # 只取最后一个时间步的输出\n",
    "        combined_output = torch.cat([\n",
    "            event_output[:, -1:, :],  # 只取最后一步\n",
    "            time_output[:, -1:, :]    # 只取最后一步\n",
    "        ], dim=-1)\n",
    "        \n",
    "        time_pred = self.time_output(combined_output)\n",
    "        mark_logits = self.mark_output(combined_output)\n",
    "        return time_pred, mark_logits\n",
    "\n",
    "    def compute_loss(self, time_pred, mark_logits, targets):\n",
    "        # 只取最后一步的真实值\n",
    "        true_times = targets[..., -1:, 1].unsqueeze(-1)  # 最后一步的时间\n",
    "        true_marks = targets[..., -1, 0].long()          # 最后一步的标记\n",
    "        \n",
    "        # 计算损失\n",
    "        time_loss = F.mse_loss(time_pred, true_times)\n",
    "        mark_logits_flat = mark_logits.view(-1, self.num_classes)\n",
    "        true_marks_flat = true_marks.view(-1)\n",
    "        mark_loss = F.cross_entropy(mark_logits_flat, true_marks_flat)\n",
    "        \n",
    "        total_loss = mark_loss + self.reg * time_loss\n",
    "        return total_loss, mark_loss, time_loss\n",
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    print(f\"Training on {device}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        train_mark_loss_sum = 0\n",
    "        train_time_loss_sum = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            batch_mark_loss = 0\n",
    "            batch_time_loss = 0\n",
    "            \n",
    "            for x, y in batch:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                x = x.unsqueeze(0)  # 添加batch维度\n",
    "                y = y.unsqueeze(0)  # 添加batch维度\n",
    "                \n",
    "                # 前向传播\n",
    "                time_pred, mark_logits = model(x)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss, mark_loss, time_loss = model.compute_loss(time_pred, mark_logits, y)\n",
    "                \n",
    "                batch_loss += loss\n",
    "                batch_mark_loss += mark_loss\n",
    "                batch_time_loss += time_loss\n",
    "            \n",
    "            batch_size = len(batch)\n",
    "            batch_loss /= batch_size\n",
    "            batch_mark_loss /= batch_size\n",
    "            batch_time_loss /= batch_size\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_total_loss += batch_loss.item()\n",
    "            train_mark_loss_sum += batch_mark_loss.item()\n",
    "            train_time_loss_sum += batch_time_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{batch_loss.item():.4f}',\n",
    "                'mark_loss': f'{batch_mark_loss.item():.4f}',\n",
    "                'time_loss': f'{batch_time_loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_total_loss / num_batches\n",
    "        avg_train_mark_loss = train_mark_loss_sum / num_batches\n",
    "        avg_train_time_loss = train_time_loss_sum / num_batches\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Mark: {avg_train_mark_loss:.4f}, Time: {avg_train_time_loss:.4f})')\n",
    "        print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/40 [00:00<?, ?it/s]/tmp/ipykernel_897/3030971952.py:81: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  time_loss = F.mse_loss(time_pred, true_times)\n",
      "Epoch 1/20: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=31.2713, mark_loss=1.9048, time_loss=293.6648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20 - 17.27s\n",
      "Train Loss: 38.3748 (Mark: 1.9589, Time: 364.1587)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=18.8100, mark_loss=1.9452, time_loss=168.6483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20 - 16.75s\n",
      "Train Loss: 24.0759 (Mark: 1.9490, Time: 221.2686)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=12.6471, mark_loss=1.9403, time_loss=107.0675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20 - 17.04s\n",
      "Train Loss: 15.3684 (Mark: 1.9475, Time: 134.2088)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 40/40 [00:17<00:00,  2.29it/s, loss=8.4767, mark_loss=1.9214, time_loss=65.5524] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20 - 17.44s\n",
      "Train Loss: 10.2832 (Mark: 1.9484, Time: 83.3486)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 40/40 [00:16<00:00,  2.36it/s, loss=5.7599, mark_loss=1.9443, time_loss=38.1554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20 - 16.98s\n",
      "Train Loss: 6.9916 (Mark: 1.9488, Time: 50.4282)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=4.1817, mark_loss=1.9420, time_loss=22.3973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20 - 16.73s\n",
      "Train Loss: 4.8962 (Mark: 1.9470, Time: 29.4924)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 40/40 [00:17<00:00,  2.30it/s, loss=3.1453, mark_loss=1.9553, time_loss=11.8994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20 - 17.41s\n",
      "Train Loss: 3.6026 (Mark: 1.9491, Time: 16.5354)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 40/40 [00:17<00:00,  2.30it/s, loss=2.6063, mark_loss=1.9619, time_loss=6.4441] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20 - 17.37s\n",
      "Train Loss: 2.8365 (Mark: 1.9489, Time: 8.8765)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 40/40 [00:16<00:00,  2.35it/s, loss=2.2949, mark_loss=1.9733, time_loss=3.2159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20 - 16.99s\n",
      "Train Loss: 2.4052 (Mark: 1.9506, Time: 4.5455)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 40/40 [00:17<00:00,  2.34it/s, loss=2.1100, mark_loss=1.9587, time_loss=1.5127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20 - 17.10s\n",
      "Train Loss: 2.1698 (Mark: 1.9477, Time: 2.2215)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=2.0280, mark_loss=1.9535, time_loss=0.7450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/20 - 16.88s\n",
      "Train Loss: 2.0525 (Mark: 1.9484, Time: 1.0405)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=1.9570, mark_loss=1.9284, time_loss=0.2856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/20 - 17.00s\n",
      "Train Loss: 1.9969 (Mark: 1.9501, Time: 0.4683)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=1.9534, mark_loss=1.9391, time_loss=0.1428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/20 - 17.03s\n",
      "Train Loss: 1.9708 (Mark: 1.9500, Time: 0.2083)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=1.9402, mark_loss=1.9333, time_loss=0.0691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/20 - 16.66s\n",
      "Train Loss: 1.9572 (Mark: 1.9477, Time: 0.0956)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=1.9308, mark_loss=1.9263, time_loss=0.0451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/20 - 17.24s\n",
      "Train Loss: 1.9526 (Mark: 1.9477, Time: 0.0494)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 40/40 [00:16<00:00,  2.36it/s, loss=1.9539, mark_loss=1.9516, time_loss=0.0235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/20 - 16.93s\n",
      "Train Loss: 1.9525 (Mark: 1.9494, Time: 0.0311)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=1.9468, mark_loss=1.9441, time_loss=0.0274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/20 - 16.69s\n",
      "Train Loss: 1.9521 (Mark: 1.9497, Time: 0.0247)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=1.9644, mark_loss=1.9629, time_loss=0.0151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/20 - 17.24s\n",
      "Train Loss: 1.9509 (Mark: 1.9487, Time: 0.0222)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=1.9407, mark_loss=1.9393, time_loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/20 - 16.87s\n",
      "Train Loss: 1.9505 (Mark: 1.9484, Time: 0.0214)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=1.9477, mark_loss=1.9445, time_loss=0.0323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/20 - 16.69s\n",
      "Train Loss: 1.9512 (Mark: 1.9490, Time: 0.0216)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = SimpleGRU(\n",
    "    num_classes=DIM_SIZE,\n",
    "    hidden_size_event=32,\n",
    "    hidden_size_time=32,\n",
    "    reg=0.1\n",
    ")\n",
    "\n",
    "# 设置训练参数\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 训练模型\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=dataloader,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# # 可选：保存模型\n",
    "# torch.save(model.state_dict(), 'simple_gru_model.pth')\n",
    "\n",
    "# # 可选：预测示例\n",
    "# model.eval()  # 设置为评估模式\n",
    "# with torch.no_grad():\n",
    "#     # 获取一个样本\n",
    "#     x_sample, y_sample = next(iter(dataloader))[0]\n",
    "#     x_sample = x_sample.unsqueeze(0)  # 添加batch维度\n",
    "    \n",
    "#     # 预测\n",
    "#     time_pred, mark_logits = model(x_sample)\n",
    "    \n",
    "#     # 获取预测的标记类别\n",
    "#     predicted_mark = torch.argmax(mark_logits, dim=-1)\n",
    "    \n",
    "#     print(\"\\n预测示例:\")\n",
    "#     print(f\"实际值 - 标记: {y_sample[0].item()}, 时间: {y_sample[1].item():.4f}\")\n",
    "#     print(f\"预测值 - 标记: {predicted_mark.item()}, 时间: {time_pred.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "标记分布:\n",
      "标记 0: 10000.0 个样本 (100.00%)\n",
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/40 [00:00<?, ?it/s]/tmp/ipykernel_897/82287682.py:55: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  time_loss = F.mse_loss(time_pred, true_times)\n",
      "Epoch 1/50: 100%|██████████| 40/40 [00:17<00:00,  2.34it/s, loss=854.1581, mark_loss=0.0000, time_loss=8541.5811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 - 17.07s\n",
      "Train Loss: 930.8562 (Mark: 0.0000, Time: 9308.5615)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=737.1065, mark_loss=0.0000, time_loss=7371.0649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 - 16.70s\n",
      "Train Loss: 794.5440 (Mark: 0.0000, Time: 7945.4399)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=648.7274, mark_loss=0.0000, time_loss=6487.2725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 - 16.85s\n",
      "Train Loss: 692.9558 (Mark: 0.0000, Time: 6929.5579)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=566.6910, mark_loss=0.0000, time_loss=5666.9106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 - 17.15s\n",
      "Train Loss: 606.3584 (Mark: 0.0000, Time: 6063.5839)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=496.5091, mark_loss=0.0000, time_loss=4965.0903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 - 16.64s\n",
      "Train Loss: 529.7967 (Mark: 0.0000, Time: 5297.9669)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=433.0239, mark_loss=0.0000, time_loss=4330.2383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 - 17.23s\n",
      "Train Loss: 461.0504 (Mark: 0.0000, Time: 4610.5034)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=372.3096, mark_loss=0.0000, time_loss=3723.0957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 - 16.73s\n",
      "Train Loss: 400.2792 (Mark: 0.0000, Time: 4002.7918)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 40/40 [00:17<00:00,  2.34it/s, loss=320.3572, mark_loss=0.0000, time_loss=3203.5718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 - 17.10s\n",
      "Train Loss: 346.6920 (Mark: 0.0000, Time: 3466.9202)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 40/40 [00:17<00:00,  2.30it/s, loss=280.7843, mark_loss=0.0000, time_loss=2807.8435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 - 17.39s\n",
      "Train Loss: 299.3643 (Mark: 0.0000, Time: 2993.6431)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=240.4721, mark_loss=0.0000, time_loss=2404.7212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 - 17.02s\n",
      "Train Loss: 257.3524 (Mark: 0.0000, Time: 2573.5236)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 40/40 [00:16<00:00,  2.42it/s, loss=204.2225, mark_loss=0.0000, time_loss=2042.2250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 - 16.53s\n",
      "Train Loss: 220.2557 (Mark: 0.0000, Time: 2202.5573)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=171.1792, mark_loss=0.0000, time_loss=1711.7919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 - 16.82s\n",
      "Train Loss: 187.6014 (Mark: 0.0000, Time: 1876.0140)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 40/40 [00:16<00:00,  2.41it/s, loss=149.6371, mark_loss=0.0000, time_loss=1496.3708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 - 16.59s\n",
      "Train Loss: 159.1729 (Mark: 0.0000, Time: 1591.7288)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=125.1671, mark_loss=0.0000, time_loss=1251.6710]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50 - 16.72s\n",
      "Train Loss: 134.2412 (Mark: 0.0000, Time: 1342.4120)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=102.7341, mark_loss=0.0000, time_loss=1027.3405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 - 16.66s\n",
      "Train Loss: 112.5792 (Mark: 0.0000, Time: 1125.7923)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 40/40 [00:16<00:00,  2.42it/s, loss=86.8478, mark_loss=0.0000, time_loss=868.4777] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50 - 16.55s\n",
      "Train Loss: 93.9625 (Mark: 0.0000, Time: 939.6250)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=70.1366, mark_loss=0.0000, time_loss=701.3661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50 - 16.66s\n",
      "Train Loss: 77.9060 (Mark: 0.0000, Time: 779.0600)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 40/40 [00:17<00:00,  2.34it/s, loss=57.2267, mark_loss=0.0000, time_loss=572.2666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50 - 17.08s\n",
      "Train Loss: 64.2353 (Mark: 0.0000, Time: 642.3525)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=48.7136, mark_loss=0.0000, time_loss=487.1362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50 - 16.67s\n",
      "Train Loss: 52.6955 (Mark: 0.0000, Time: 526.9551)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=37.6529, mark_loss=0.0000, time_loss=376.5287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50 - 16.88s\n",
      "Train Loss: 42.8715 (Mark: 0.0000, Time: 428.7148)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=30.3359, mark_loss=0.0000, time_loss=303.3589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50 - 17.14s\n",
      "Train Loss: 34.6914 (Mark: 0.0000, Time: 346.9142)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=25.5730, mark_loss=0.0000, time_loss=255.7302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50 - 16.72s\n",
      "Train Loss: 27.9191 (Mark: 0.0000, Time: 279.1907)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=20.1069, mark_loss=0.0000, time_loss=201.0686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50 - 17.21s\n",
      "Train Loss: 22.2852 (Mark: 0.0000, Time: 222.8518)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=16.7291, mark_loss=0.0000, time_loss=167.2906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50 - 16.85s\n",
      "Train Loss: 17.6905 (Mark: 0.0000, Time: 176.9049)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=11.6511, mark_loss=0.0000, time_loss=116.5115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50 - 16.75s\n",
      "Train Loss: 13.8944 (Mark: 0.0000, Time: 138.9439)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=10.3248, mark_loss=0.0000, time_loss=103.2481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50 - 17.13s\n",
      "Train Loss: 10.8956 (Mark: 0.0000, Time: 108.9556)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 40/40 [00:17<00:00,  2.31it/s, loss=6.9217, mark_loss=0.0000, time_loss=69.2171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50 - 17.32s\n",
      "Train Loss: 8.4345 (Mark: 0.0000, Time: 84.3451)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=5.8484, mark_loss=0.0000, time_loss=58.4843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50 - 17.03s\n",
      "Train Loss: 6.5178 (Mark: 0.0000, Time: 65.1781)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 40/40 [00:17<00:00,  2.31it/s, loss=4.3127, mark_loss=0.0000, time_loss=43.1269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50 - 17.30s\n",
      "Train Loss: 4.9888 (Mark: 0.0000, Time: 49.8876)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 40/40 [00:17<00:00,  2.32it/s, loss=3.3905, mark_loss=0.0000, time_loss=33.9053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50 - 17.22s\n",
      "Train Loss: 3.7994 (Mark: 0.0000, Time: 37.9937)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=2.2414, mark_loss=0.0000, time_loss=22.4138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50 - 16.70s\n",
      "Train Loss: 2.8662 (Mark: 0.0000, Time: 28.6619)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 40/40 [00:16<00:00,  2.36it/s, loss=1.8880, mark_loss=0.0000, time_loss=18.8796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50 - 16.91s\n",
      "Train Loss: 2.1617 (Mark: 0.0000, Time: 21.6175)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 40/40 [00:16<00:00,  2.41it/s, loss=1.2874, mark_loss=0.0000, time_loss=12.8739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50 - 16.63s\n",
      "Train Loss: 1.6166 (Mark: 0.0000, Time: 16.1661)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 40/40 [00:16<00:00,  2.36it/s, loss=1.0074, mark_loss=0.0000, time_loss=10.0738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50 - 16.93s\n",
      "Train Loss: 1.2093 (Mark: 0.0000, Time: 12.0931)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=0.5666, mark_loss=0.0000, time_loss=5.6662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50 - 16.89s\n",
      "Train Loss: 0.8985 (Mark: 0.0000, Time: 8.9848)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.6241, mark_loss=0.0000, time_loss=6.2410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50 - 16.80s\n",
      "Train Loss: 0.6773 (Mark: 0.0000, Time: 6.7733)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 40/40 [00:17<00:00,  2.27it/s, loss=0.4454, mark_loss=0.0000, time_loss=4.4542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50 - 17.60s\n",
      "Train Loss: 0.5091 (Mark: 0.0000, Time: 5.0905)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=0.4361, mark_loss=0.0000, time_loss=4.3609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50 - 17.14s\n",
      "Train Loss: 0.3892 (Mark: 0.0000, Time: 3.8923)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 40/40 [00:16<00:00,  2.36it/s, loss=0.2379, mark_loss=0.0000, time_loss=2.3791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/50 - 16.93s\n",
      "Train Loss: 0.2981 (Mark: 0.0000, Time: 2.9814)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 40/40 [00:17<00:00,  2.35it/s, loss=0.1654, mark_loss=0.0000, time_loss=1.6543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/50 - 17.05s\n",
      "Train Loss: 0.2348 (Mark: 0.0000, Time: 2.3484)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=0.1743, mark_loss=0.0000, time_loss=1.7432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/50 - 16.85s\n",
      "Train Loss: 0.1916 (Mark: 0.0000, Time: 1.9163)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=0.1224, mark_loss=0.0000, time_loss=1.2241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/50 - 16.85s\n",
      "Train Loss: 0.1599 (Mark: 0.0000, Time: 1.5992)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.1005, mark_loss=0.0000, time_loss=1.0052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/50 - 16.81s\n",
      "Train Loss: 0.1384 (Mark: 0.0000, Time: 1.3843)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.0883, mark_loss=0.0000, time_loss=0.8831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/50 - 16.78s\n",
      "Train Loss: 0.1237 (Mark: 0.0000, Time: 1.2368)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 40/40 [00:16<00:00,  2.37it/s, loss=0.1994, mark_loss=0.0000, time_loss=1.9936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/50 - 16.89s\n",
      "Train Loss: 0.1166 (Mark: 0.0000, Time: 1.1660)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 40/40 [00:17<00:00,  2.33it/s, loss=0.1706, mark_loss=0.0000, time_loss=1.7058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/50 - 17.14s\n",
      "Train Loss: 0.1095 (Mark: 0.0000, Time: 1.0952)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.1070, mark_loss=0.0000, time_loss=1.0696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/50 - 16.82s\n",
      "Train Loss: 0.1039 (Mark: 0.0000, Time: 1.0385)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 40/40 [00:17<00:00,  2.28it/s, loss=0.0732, mark_loss=0.0000, time_loss=0.7318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/50 - 17.56s\n",
      "Train Loss: 0.1001 (Mark: 0.0000, Time: 1.0014)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.1994, mark_loss=0.0000, time_loss=1.9941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/50 - 16.81s\n",
      "Train Loss: 0.1012 (Mark: 0.0000, Time: 1.0116)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 40/40 [00:16<00:00,  2.39it/s, loss=0.0950, mark_loss=0.0000, time_loss=0.9498]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/50 - 16.73s\n",
      "Train Loss: 0.0976 (Mark: 0.0000, Time: 0.9761)\n",
      "\n",
      "预测示例:\n",
      "实际标记: 0.0\n",
      "预测标记: 0\n",
      "标记概率分布: 1.0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 198\u001b[0m\n\u001b[1;32m    195\u001b[0m x_sample, y_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    196\u001b[0m x_sample \u001b[38;5;241m=\u001b[39m x_sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m time_pred, mark_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m predicted_mark \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(mark_logits\u001b[38;5;241m.\u001b[39msqueeze(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m最终预测示例:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 38\u001b[0m, in \u001b[0;36mSimpleGRU.forward\u001b[0;34m(self, event_sequence)\u001b[0m\n\u001b[1;32m     35\u001b[0m marks \u001b[38;5;241m=\u001b[39m event_sequence[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     36\u001b[0m times \u001b[38;5;241m=\u001b[39m event_sequence[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m mark_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m event_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_gru(mark_embedded)\n\u001b[1;32m     40\u001b[0m time_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_gru(times)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) >= 2:  # 确保序列至少有2个事件\n",
    "                # 将序列转换为tensor\n",
    "                seq_tensor = torch.tensor([[event[0], event[1]] for event in seq], dtype=torch.float32)\n",
    "                # 分离输入序列(x)和目标值(y)\n",
    "                x = seq_tensor[:-1]  # 除最后一个外的所有事件\n",
    "                y = seq_tensor[-1]   # 最后一个事件\n",
    "                self.sequences.append((x, y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size_event, hidden_size_time, reg=0.1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg = reg\n",
    "        self.event_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        self.event_gru = nn.GRU(num_classes, hidden_size_event, batch_first=True)\n",
    "        self.time_gru = nn.GRU(1, hidden_size_time, batch_first=True)\n",
    "        combined_size = hidden_size_event + hidden_size_time\n",
    "        self.time_output = nn.Linear(combined_size, 1)\n",
    "        self.mark_output = nn.Linear(combined_size, num_classes)\n",
    "\n",
    "    def forward(self, event_sequence):\n",
    "        marks = event_sequence[..., 0].long()\n",
    "        times = event_sequence[..., 1].unsqueeze(-1)\n",
    "        \n",
    "        mark_embedded = self.event_embedding(marks)\n",
    "        event_output, _ = self.event_gru(mark_embedded)\n",
    "        time_output, _ = self.time_gru(times)\n",
    "        \n",
    "        combined_output = torch.cat([\n",
    "            event_output[:, -1, :],\n",
    "            time_output[:, -1, :]\n",
    "        ], dim=-1)\n",
    "        \n",
    "        time_pred = self.time_output(combined_output).unsqueeze(1)\n",
    "        mark_logits = self.mark_output(combined_output).unsqueeze(1)\n",
    "        return time_pred, mark_logits\n",
    "\n",
    "    def compute_loss(self, time_pred, mark_logits, targets):\n",
    "        true_times = targets[..., 1].unsqueeze(-1)\n",
    "        true_marks = targets[..., 0].long()\n",
    "        \n",
    "        time_loss = F.mse_loss(time_pred, true_times)\n",
    "        mark_logits_flat = mark_logits.squeeze(1)  # 移除多余的维度\n",
    "        mark_loss = F.cross_entropy(mark_logits_flat, true_marks)\n",
    "        \n",
    "        total_loss = mark_loss + self.reg * time_loss\n",
    "        return total_loss, mark_loss, time_loss\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    print(f\"Training on {device}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        train_mark_loss_sum = 0\n",
    "        train_time_loss_sum = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            batch_mark_loss = 0\n",
    "            batch_time_loss = 0\n",
    "            \n",
    "            for x, y in batch:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                x = x.unsqueeze(0)  # 添加batch维度\n",
    "                y = y.unsqueeze(0)  # 添加batch维度\n",
    "                \n",
    "                # 前向传播\n",
    "                time_pred, mark_logits = model(x)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss, mark_loss, time_loss = model.compute_loss(time_pred, mark_logits, y)\n",
    "                \n",
    "                batch_loss += loss\n",
    "                batch_mark_loss += mark_loss\n",
    "                batch_time_loss += time_loss\n",
    "            \n",
    "            batch_size = len(batch)\n",
    "            batch_loss /= batch_size\n",
    "            batch_mark_loss /= batch_size\n",
    "            batch_time_loss /= batch_size\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_total_loss += batch_loss.item()\n",
    "            train_mark_loss_sum += batch_mark_loss.item()\n",
    "            train_time_loss_sum += batch_time_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{batch_loss.item():.4f}',\n",
    "                'mark_loss': f'{batch_mark_loss.item():.4f}',\n",
    "                'time_loss': f'{batch_time_loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_total_loss / num_batches\n",
    "        avg_train_mark_loss = train_mark_loss_sum / num_batches\n",
    "        avg_train_time_loss = train_time_loss_sum / num_batches\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Mark: {avg_train_mark_loss:.4f}, Time: {avg_train_time_loss:.4f})')\n",
    "        \n",
    "        # 每5个epoch打印预测示例\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x_sample, y_sample = next(iter(train_loader))[0]\n",
    "                x_sample = x_sample.to(device)\n",
    "                x_sample = x_sample.unsqueeze(0)\n",
    "                \n",
    "                time_pred, mark_logits = model(x_sample)\n",
    "                predicted_mark = torch.argmax(mark_logits.squeeze(), dim=-1)\n",
    "                \n",
    "                print(\"\\n预测示例:\")\n",
    "                print(f\"实际标记: {y_sample[0].item()}\")\n",
    "                print(f\"预测标记: {predicted_mark.item()}\")\n",
    "                print(f\"标记概率分布: {torch.softmax(mark_logits.squeeze(), dim=-1)}\")\n",
    "            model.train()\n",
    "        \n",
    "        print('-' * 80)\n",
    "\n",
    "# 生成数据\n",
    "DIM_SIZE = 1\n",
    "BATCH_SIZE = 256\n",
    "mi = MarkedIntensityHomogenuosPoisson(DIM_SIZE)\n",
    "for u in range(DIM_SIZE):\n",
    "    mi.initialize(1.0, u)\n",
    "simulated_sequences = generate_samples_marked(mi, 100.0, 10000)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = PointProcessDataset(simulated_sequences)\n",
    "\n",
    "# 检查数据分布\n",
    "mark_counts = torch.zeros(DIM_SIZE)\n",
    "for x, y in dataset:\n",
    "    mark_counts[int(y[0])] += 1\n",
    "\n",
    "print(\"\\n标记分布:\")\n",
    "for i in range(DIM_SIZE):\n",
    "    print(f\"标记 {i}: {mark_counts[i]} 个样本 ({mark_counts[i]/len(dataset)*100:.2f}%)\")\n",
    "\n",
    "# 创建数据加载器\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 创建和训练模型\n",
    "model = SimpleGRU(\n",
    "    num_classes=DIM_SIZE,\n",
    "    hidden_size_event=64,\n",
    "    hidden_size_time=64,\n",
    "    reg=0.1\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=dataloader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'simple_gru_model.pth')\n",
    "\n",
    "# 测试预测\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_sample, y_sample = next(iter(dataloader))[0]\n",
    "    x_sample = x_sample.unsqueeze(0)\n",
    "    \n",
    "    time_pred, mark_logits = model(x_sample)\n",
    "    predicted_mark = torch.argmax(mark_logits.squeeze(), dim=-1)\n",
    "    \n",
    "    print(\"\\n最终预测示例:\")\n",
    "    print(f\"实际值 - 标记: {y_sample[0].item()}, 时间: {y_sample[1].item():.4f}\")\n",
    "    print(f\"预测值 - 标记: {predicted_mark.item()}, 时间: {time_pred.squeeze().item():.4f}\")"
=======
      "Epoch 10/20\n",
      "Average Train Loss: 1.6929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]:  57%|█████▋    | 178/312 [00:00<00:00, 199.47it/s, loss=1.7709]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 219\u001b[0m\n\u001b[1;32m    211\u001b[0m     model \u001b[38;5;241m=\u001b[39m GRUPointProcess(\n\u001b[1;32m    212\u001b[0m         time_step\u001b[38;5;241m=\u001b[39mtime_step,\n\u001b[1;32m    213\u001b[0m         size_gru\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    214\u001b[0m         size_nn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    215\u001b[0m         size_layer_chfn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# 检查数据形状\u001b[39;00m\n\u001b[1;32m    228\u001b[0m sample_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n",
      "Cell \u001b[0;32mIn[153], line 151\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    148\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    150\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43melapsed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43melapsed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:697\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m~/ENTER/envs/pymc_env/lib/python3.12/site-packages/torch/autograd/profiler.py:738\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_enter_new(\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\n\u001b[1;32m    735\u001b[0m     )\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit:\n\u001b[1;32m    740\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 生成训练数据\n",
    "def generate_data(num_samples, lambda_=0.5):\n",
    "    # 生成指数分布的时间间隔\n",
    "    intervals = np.random.exponential(1/lambda_, num_samples)\n",
    "    # 计算累积时间\n",
    "    times = np.cumsum(intervals)\n",
    "    return times\n",
    "\n",
    "# 数据集类\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, data, time_step=20):\n",
    "        # 确保数据是按时间排序的\n",
    "        self.data = np.sort(data)\n",
    "        self.time_step = time_step\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.time_step\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # 获取时间序列窗口\n",
    "        history = self.data[idx:idx+self.time_step].reshape(-1, 1)\n",
    "        next_time = self.data[idx+self.time_step]\n",
    "        \n",
    "        # 计算时间差（elapsed time）\n",
    "        elapsed = next_time - history[-1, 0]\n",
    "        \n",
    "        return {\n",
    "            'history': torch.FloatTensor(history),\n",
    "            'elapsed': torch.FloatTensor([elapsed])\n",
    "        }\n",
    "\n",
    "class GRUPointProcess(nn.Module):\n",
    "    def __init__(self, time_step=20, size_gru=64, size_nn=64, size_layer_chfn=2):\n",
    "        super(GRUPointProcess, self).__init__()\n",
    "        self.time_step = time_step\n",
    "        self.size_gru = size_gru\n",
    "        self.size_nn = size_nn\n",
    "        \n",
    "        # GRU层\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=size_gru,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        self.elapsed_time_linear = nn.Linear(1, size_nn, bias=False)\n",
    "        self.gru_linear = nn.Linear(size_gru, size_nn)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(size_nn, size_nn)\n",
    "            for _ in range(size_layer_chfn-1)\n",
    "        ])\n",
    "        \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(size_nn, 1)\n",
    "        \n",
    "        # 初始化为正权重\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 使用较小的初始值\n",
    "            nn.init.uniform_(module.weight, 0, 0.1)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, event_history, elapsed_time):\n",
    "        # 添加数值稳定性检查\n",
    "        event_history = torch.clamp(event_history, min=1e-6)\n",
    "        elapsed_time = torch.clamp(elapsed_time, min=1e-6)\n",
    "        elapsed_time = elapsed_time.requires_grad_(True)\n",
    "        \n",
    "        # 标准化输入\n",
    "        event_history_norm = event_history\n",
    "        elapsed_time_norm = elapsed_time\n",
    "        \n",
    "        # GRU处理历史序列\n",
    "        gru_out, _ = self.gru(event_history_norm)\n",
    "        gru_out = gru_out[:, -1, :]  # 取最后一个时间步的输出\n",
    "        \n",
    "        # 第一个隐藏层\n",
    "        hidden_tau = self.elapsed_time_linear(elapsed_time_norm)\n",
    "        hidden_gru = self.gru_linear(gru_out)\n",
    "        hidden = torch.tanh(hidden_tau + hidden_gru)\n",
    "        \n",
    "        # 后续隐藏层\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = torch.tanh(layer(hidden))\n",
    "        \n",
    "        # 计算累积危险函数（添加数值稳定性）\n",
    "        Int_l = F.softplus(self.output_layer(hidden)) + 1e-6\n",
    "        \n",
    "        # 计算危险函数\n",
    "        try:\n",
    "            l = torch.autograd.grad(Int_l.sum(), elapsed_time, create_graph=True)[0]\n",
    "            # 确保l是正的且有界\n",
    "            l = torch.clamp(l, min=1e-6, max=1e6)\n",
    "        except RuntimeError:\n",
    "            print(\"Gradient computation failed\")\n",
    "            l = torch.ones_like(elapsed_time) * 1e-6\n",
    "        \n",
    "        return l, Int_l\n",
    "    \n",
    "    def compute_loss(self, l, Int_l):\n",
    "        # 添加数值稳定性\n",
    "        l = torch.clamp(l, min=1e-6)\n",
    "        Int_l = torch.clamp(Int_l, min=0.0)\n",
    "        \n",
    "        # 计算负对数似然\n",
    "        loss = -torch.mean(torch.log(l) - Int_l)\n",
    "        \n",
    "        # 检查loss是否为nan\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Warning: Loss is NaN!\")\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=l.device)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "def train_model(model, train_loader, test_loader=None, num_epochs=10, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # 添加学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in train_bar:\n",
    "            history = batch['history'].to(device)\n",
    "            elapsed = batch['elapsed'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            l, Int_l = model(history, elapsed)\n",
    "            loss = model.compute_loss(l, Int_l)\n",
    "            \n",
    "            # 检查loss是否为nan\n",
    "            if not torch.isnan(loss):\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Average Train Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(avg_train_loss)\n",
    "        \n",
    "        # 保存模型\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f'gru_point_process_epoch_{epoch+1}.pt')\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    num_samples = 10000\n",
    "    T_train = generate_data(num_samples)\n",
    "    T_test = generate_data(num_samples // 10)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    time_step = 20\n",
    "    batch_size = 32\n",
    "    train_dataset = PointProcessDataset(T_train, time_step)\n",
    "    test_dataset = PointProcessDataset(T_test, time_step)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False  # 不打乱顺序，保持时间序列的连续性\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    model = GRUPointProcess(\n",
    "        time_step=time_step,\n",
    "        size_gru=32,\n",
    "        size_nn=32,\n",
    "        size_layer_chfn=2\n",
    "    )\n",
    "    \n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        num_epochs=20,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "\n",
    "# 检查数据形状\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nData shapes:\")\n",
    "print(\"History shape:\", sample_batch['history'].shape)\n",
    "print(\"Elapsed shape:\", sample_batch['elapsed'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_synthetic_data(n_events=100, lambda_rate=0.5):\n",
    "    inter_event_times = np.random.exponential(scale=1/lambda_rate, size=n_events)\n",
    "    event_times = np.cumsum(inter_event_times)\n",
    "    return event_times\n",
    "event_times = generate_synthetic_data(n_events=200)\n",
    "\n",
    "# 定义数据集\n",
    "class PointProcessDataset(Dataset):\n",
    "    def __init__(self, data, time_step=20):\n",
    "        self.data = data\n",
    "        self.time_step = time_step\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.time_step\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        history = self.data[idx:idx+self.time_step].reshape(-1, 1)\n",
    "        next_time = self.data[idx+self.time_step]\n",
    "        elapsed = next_time - history[-1, 0]\n",
    "        return {\n",
    "            'history': torch.FloatTensor(history),\n",
    "            'elapsed': torch.FloatTensor([elapsed])\n",
    "        }\n",
    "\n",
    "# 数据准备\n",
    "time_step = 20\n",
    "dataset = PointProcessDataset(event_times, time_step)\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 0.12949381914981664],\n",
       " [4, 0.3353709352659866],\n",
       " [6, 0.43187939155696436],\n",
       " [3, 0.4723014075603091],\n",
       " [2, 0.61790116373476],\n",
       " [1, 0.8399055026771673],\n",
       " [2, 0.8452440560013247],\n",
       " [6, 1.4666864515456974],\n",
       " [5, 1.7580242425621886],\n",
       " [6, 1.818028957951006],\n",
       " [1, 2.1430714118795233],\n",
       " [5, 2.2663320341081996],\n",
       " [5, 2.311670906546937],\n",
       " [2, 2.3581622260597896],\n",
       " [4, 2.481241405029868],\n",
       " [3, 2.9720434132651334],\n",
       " [2, 3.159263377802795],\n",
       " [4, 3.3186616772390862],\n",
       " [0, 3.372953715789553],\n",
       " [5, 3.6548165800921772],\n",
       " [2, 3.7653169857610287],\n",
       " [0, 3.7853703435309427],\n",
       " [3, 3.91650174770075],\n",
       " [4, 3.936981528380828],\n",
       " [4, 3.9494050274276655],\n",
       " [5, 3.9507222996889326],\n",
       " [1, 4.138771321527632],\n",
       " [6, 4.2683749522323815],\n",
       " [4, 4.3066639520853505],\n",
       " [2, 4.308695123019771],\n",
       " [0, 4.322911991324929],\n",
       " [4, 4.3928904840742895],\n",
       " [3, 4.521859336679029],\n",
       " [2, 4.838476372863003],\n",
       " [0, 4.907627370903179],\n",
       " [2, 4.920916633231023],\n",
       " [2, 5.358638371912151],\n",
       " [2, 5.802718068025694],\n",
       " [3, 5.9051101600728035],\n",
       " [1, 5.917877802432683],\n",
       " [2, 6.21684744033996],\n",
       " [4, 6.30620190343399],\n",
       " [3, 6.330530805236483],\n",
       " [3, 6.423161600032227],\n",
       " [0, 6.665434746785999],\n",
       " [1, 6.789700679154123],\n",
       " [2, 7.449387834181706],\n",
       " [3, 7.560075853302074],\n",
       " [0, 7.8923580925815715],\n",
       " [4, 8.228714495149186],\n",
       " [1, 8.303045890379238],\n",
       " [0, 8.357224863898555],\n",
       " [4, 8.44904590906441],\n",
       " [4, 8.615490489007524],\n",
       " [3, 8.674253319538582],\n",
       " [3, 8.763260802015187],\n",
       " [5, 8.764073644326219],\n",
       " [0, 8.770078695506484],\n",
       " [5, 8.89559017238852],\n",
       " [5, 9.076751349339052],\n",
       " [4, 9.092048141301733],\n",
       " [2, 9.113655811011938],\n",
       " [1, 9.121973677434783],\n",
       " [1, 9.307548566083742],\n",
       " [1, 9.866285475167134],\n",
       " [1, 9.88677342683037],\n",
       " [2, 9.98047636489197],\n",
       " [3, 10.017609527267881],\n",
       " [3, 10.022703261237732],\n",
       " [5, 10.110429944384007],\n",
       " [0, 10.36925248811159],\n",
       " [3, 10.487204520542113],\n",
       " [3, 10.950470140562597],\n",
       " [6, 11.281205312903674],\n",
       " [3, 11.406383513580133],\n",
       " [0, 11.428547563928158],\n",
       " [1, 11.590838005321007],\n",
       " [1, 11.77222587238221],\n",
       " [5, 12.071078698348135],\n",
       " [4, 12.38599366208618],\n",
       " [2, 12.445545844661172],\n",
       " [1, 12.569891921512168],\n",
       " [1, 12.580795809386625],\n",
       " [1, 12.893335628115862],\n",
       " [4, 12.903549644578865],\n",
       " [4, 13.065511541561598],\n",
       " [5, 13.167438333166613],\n",
       " [5, 13.187524391296215],\n",
       " [0, 13.327498998087686],\n",
       " [0, 13.580962526086248],\n",
       " [3, 13.81130033292606],\n",
       " [3, 13.825773579587278],\n",
       " [3, 13.828638732423737],\n",
       " [5, 13.858042994645487],\n",
       " [1, 13.890277170036736],\n",
       " [6, 13.953605965953662],\n",
       " [3, 14.048195259816975],\n",
       " [2, 14.079868272039803],\n",
       " [5, 14.3228095364217],\n",
       " [5, 14.360276189545637],\n",
       " [3, 14.53776478606831],\n",
       " [2, 15.02319260875601],\n",
       " [6, 15.081342683305241],\n",
       " [6, 15.261370962148273],\n",
       " [5, 15.349078508508361],\n",
       " [4, 15.527701461348025],\n",
       " [2, 15.93147734144383],\n",
       " [3, 16.042370959885503],\n",
       " [4, 16.10342084157525],\n",
       " [5, 16.35984607527543],\n",
       " [1, 16.416987142682466],\n",
       " [3, 16.44682603546103],\n",
       " [0, 16.4626862672014],\n",
       " [5, 16.560815088519167],\n",
       " [3, 16.61550752592137],\n",
       " [5, 17.0252970220866],\n",
       " [1, 17.291247561300313],\n",
       " [0, 17.29133788950336],\n",
       " [4, 17.336997193845708],\n",
       " [4, 17.409335135196116],\n",
       " [2, 18.797178102194476],\n",
       " [2, 18.813741026130735],\n",
       " [0, 19.103698388347603],\n",
       " [1, 19.146916162996504],\n",
       " [6, 19.213360203998946],\n",
       " [1, 19.295958932654965],\n",
       " [5, 19.304806395116977],\n",
       " [0, 19.649234647717467]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_sequences[0]"
=======
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient computation failed\n",
      "Shape of predicted intensity (l): torch.Size([2, 1])\n",
      "Shape of cumulative intensity (Int_l): torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the GRUPointProcess model\n",
    "class GRUPointProcess(nn.Module):\n",
    "    def __init__(self, time_step=4, size_gru=64, size_nn=64, size_layer_chfn=4):\n",
    "        super(GRUPointProcess, self).__init__()\n",
    "        self.time_step = time_step\n",
    "        self.size_gru = size_gru\n",
    "        self.size_nn = size_nn\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=size_gru,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.elapsed_time_linear = nn.Linear(1, size_nn, bias=False)\n",
    "        self.gru_linear = nn.Linear(size_gru, size_nn)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(size_nn, size_nn)\n",
    "            for _ in range(size_layer_chfn-1)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(size_nn, 1)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.uniform_(module.weight, 0, 0.1)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, event_history, elapsed_time):\n",
    "        event_history = torch.clamp(event_history, min=1e-6)\n",
    "        elapsed_time = torch.clamp(elapsed_time, min=1e-6)\n",
    "        elapsed_time = elapsed_time.requires_grad_(True)\n",
    "        gru_out, _ = self.gru(event_history)\n",
    "        gru_out = gru_out[:, -1, :]\n",
    "        hidden_tau = self.elapsed_time_linear(elapsed_time)\n",
    "        hidden_gru = self.gru_linear(gru_out)\n",
    "        hidden = torch.tanh(hidden_tau + hidden_gru)\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = torch.tanh(layer(hidden))\n",
    "        Int_l = F.softplus(self.output_layer(hidden))\n",
    "        try:\n",
    "            l = torch.autograd.grad(Int_l, elapsed_time, create_graph=True)[0]\n",
    "            l = torch.clamp(l, min=1e-6, max=1e6)\n",
    "        except RuntimeError:\n",
    "            print(\"Gradient computation failed\")\n",
    "            l = torch.ones_like(elapsed_time) * 1e-6\n",
    "        return l, Int_l\n",
    "    \n",
    "    def compute_loss(self, l, Int_l):\n",
    "        l = torch.clamp(l, min=1e-6)\n",
    "        Int_l = torch.clamp(Int_l, min=0.0)\n",
    "        loss = -torch.mean(torch.log(l) - Int_l)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Warning: Loss is NaN!\")\n",
    "            loss = torch.tensor(0.0, requires_grad=True, device=l.device)\n",
    "            \n",
    "        return loss\n",
    "\n",
    "# Create some dummy data\n",
    "event_history = torch.randn(2, 4, 1)  # 2 samples, 4 time steps, 1 feature\n",
    "elapsed_time = torch.randn(2, 1)  # 2 samples, 1 elapsed time value\n",
    "\n",
    "# Instantiate and run the model\n",
    "model = GRUPointProcess(time_step=4, size_gru=64, size_nn=64, size_layer_chfn=2)\n",
    "l, Int_l = model(event_history, elapsed_time)\n",
    "\n",
    "# Print the output shapes\n",
    "print(f\"Shape of predicted intensity (l): {l.shape}\")\n",
    "print(f\"Shape of cumulative intensity (Int_l): {Int_l.shape}\")\n"
>>>>>>> 11d936a657fe2daccfc42b5a872d7de2a28291f4
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
